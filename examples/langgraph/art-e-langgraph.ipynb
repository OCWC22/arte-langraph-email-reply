{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OCWC22/arte-langraph-email-reply/blob/main/examples/langgraph/art-e-langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emg-y6MMIX6D"
      },
      "source": [
        "To train this email search agent using LangGraph, click **Runtime** > **Run all**. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**Email Search Agent with LangGraph**\n",
        "\n",
        "In this notebook, you will be using [ART](https://github.com/openpipe/art) together with [LangGraph](https://langchain-ai.github.io/langgraph/) to train your own ARTâ€¢E agent from scratch! This implementation demonstrates how to integrate LangGraph's agent framework with ART's training capabilities.\n",
        "\n",
        "Beginning with a Qwen 2.5 7B base model, you will train it to search through emails and answer questions about them using LangGraph's ReAct agent pattern. You will construct an [agentic environment](#Environment), define a [rollout](#Rollout) using LangGraph, and run a [training loop](#Loop). You will also learn how to use [RULER](#ruler) to judge the quality of the agent's answers.\n",
        "\n",
        "**RULER**\n",
        "\n",
        "RULER is a robust technique for evaluating the quality of an agent's answers and training the agent to produce more of its best completions. To learn more about RULER, see the [RULER documentation](https://art.openpipe.ai/fundamentals/ruler).\n",
        "\n",
        "Now let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QBijDl7iIX6F"
      },
      "outputs": [],
      "source": [
        "#@title ðŸ’¿ Installation\n",
        "\n",
        "# Portions adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks)\n",
        "# Copyright (c) Unsloth contributors.\n",
        "# License: GNU LGPL v3.0.\n",
        "# Modifications by OpenPipe:\n",
        "# - switched to uv\n",
        "# - changed vllm/triton pinning logic\n",
        "# - added litellm/protobuf pins\n",
        "# See /licenses/LGPL-3.0.txt and /licenses/GPL-3.0.txt for full text.\n",
        "\n",
        "%%capture\n",
        "import os\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !uv pip install \"openpipe-art[backend,langgraph]==0.4.9\" langchain-core langgraph langchain_openai tenacity datasets \"gql<4\" --prerelease allow --no-cache-dir\n",
        "else:\n",
        "    try:\n",
        "        import numpy\n",
        "\n",
        "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except:\n",
        "        get_numpy = \"numpy\"\n",
        "    try:\n",
        "        import subprocess\n",
        "\n",
        "        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except:\n",
        "        is_t4 = False\n",
        "    get_vllm, get_triton = (\n",
        "        (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
        "    )\n",
        "    !uv pip install --upgrade \\\n",
        "        \"openpipe-art[backend,langgraph]==0.4.9\" langchain-core langgraph langchain_openai tenacity datasets \"gql<4\" \"protobuf==5.29.5\" {get_vllm} {get_numpy} --prerelease allow --no-cache-dir\n",
        "    !uv pip install -qqq {get_triton}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOMCNkBVIX6G"
      },
      "source": [
        "<a name=\"Environment-Variables\"></a>\n",
        "\n",
        "### Environment Variables\n",
        "\n",
        "**OpenAI (used for RULER judge model)**\n",
        "\n",
        "Our RULER reward function queries third-party models to judge the quality of the agent's performance. Any model supported by LiteLLM works. For this example we'll use OpenAI's o4-mini model, so we'll need to set the `OPENAI_API_KEY` environment variable.\n",
        "\n",
        "**Weights & Biases (optional)**\n",
        "\n",
        "Later on in the notebook, we'll be creating a model that can automatically logs metrics to Weights & Biases and chat completions to Weave. In order to do so, you'll need to provide your Weights & Biases API key as an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx9dP2GNIX6G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Required\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY is required for RULER functionality when using openai/o4-mini.\"\n",
        "    )\n",
        "\n",
        "# Optional\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n",
        "if not os.environ.get(\"WANDB_API_KEY\"):\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9QaTu1CIX6G"
      },
      "source": [
        "<a name=\"Environment\"></a>\n",
        "\n",
        "### Email Search Environment\n",
        "\n",
        "ART allows your agent to learn by interacting with its environment. In this example, we'll create an environment where the agent can search through emails and answer questions about them using LangGraph's tools integration.\n",
        "\n",
        "The agent will have access to three tools:\n",
        "\n",
        "1. `search_inbox` - Search for emails by keywords\n",
        "2. `read_email` - Read a specific email by message ID\n",
        "3. `return_final_answer` - Return the final answer with source email IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9Xvmi-yIX6H",
        "outputId": "d43031fc-4ba8-4d1b-a941-b80462ed519c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading train scenarios from Hugging Face...\n",
            "Loaded 50 scenarios.\n",
            "Email search environment created with full Enron dataset!\n",
            "Database contains the complete email dataset, loaded 50 training scenarios.\n",
            "\n",
            "Sample scenario\n",
            "id: 3296\n",
            "question: Who can I contact for Power Operations when Sally is in London?\n",
            "answer: Stacey White (x31870) and Leslie Reeves (x37962).\n",
            "message_ids: ['<6033065.1075856098960.JavaMail.evans@thyme>']\n",
            "how_realistic: 0.699999988079071\n",
            "inbox_address: louise.kitchen@enron.com\n",
            "query_date: 2001-01-25\n",
            "split: train\n"
          ]
        }
      ],
      "source": [
        "#@title Email Search Code\n",
        "\n",
        "import os\n",
        "import random\n",
        "import sqlite3\n",
        "from dataclasses import asdict, dataclass\n",
        "from datetime import datetime\n",
        "from textwrap import dedent\n",
        "from typing import List, Literal, Optional\n",
        "\n",
        "from datasets import Dataset, Features, Sequence, Value, load_dataset\n",
        "from pydantic import BaseModel, Field\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Email and Scenario data models\n",
        "class Email(BaseModel):\n",
        "    message_id: str\n",
        "    date: str  # ISO 8601 string 'YYYY-MM-DD HH:MM:SS'\n",
        "    subject: Optional[str] = None\n",
        "    from_address: Optional[str] = None\n",
        "    to_addresses: List[str] = []  # Populated from recipients table\n",
        "    cc_addresses: List[str] = []  # Populated from recipients table\n",
        "    bcc_addresses: List[str] = []  # Populated from recipients table\n",
        "    body: Optional[str] = None\n",
        "    file_name: Optional[str] = None\n",
        "\n",
        "\n",
        "class Scenario(BaseModel):\n",
        "    id: int\n",
        "    question: str\n",
        "    answer: str\n",
        "    message_ids: List[str]  # message_ids (strings) of referenced emails\n",
        "    how_realistic: float\n",
        "    inbox_address: str\n",
        "    query_date: str\n",
        "    split: Literal[\"train\", \"test\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    message_id: str\n",
        "    snippet: str\n",
        "\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    answer: str\n",
        "    source_ids: list[str]\n",
        "\n",
        "\n",
        "# Database configuration\n",
        "DB_PATH = \"./enron_emails.db\"\n",
        "EMAIL_DATASET_REPO_ID = \"corbt/enron-emails\"\n",
        "SCENARIO_DATASET_REPO_ID = \"corbt/enron_emails_sample_questions\"\n",
        "\n",
        "# Global database connection\n",
        "db_conn = None\n",
        "\n",
        "\n",
        "def create_email_database():\n",
        "    \"\"\"Create the email database from Hugging Face dataset\"\"\"\n",
        "    print(\"Creating email database from Hugging Face dataset...\")\n",
        "    print(\n",
        "        \"This will download and process the full Enron email dataset - this may take several minutes...\"\n",
        "    )\n",
        "\n",
        "    # Database schema\n",
        "    SQL_CREATE_TABLES = \"\"\"\n",
        "    DROP TABLE IF EXISTS recipients;\n",
        "    DROP TABLE IF EXISTS emails_fts;\n",
        "    DROP TABLE IF EXISTS emails;\n",
        "\n",
        "    CREATE TABLE emails (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        message_id TEXT UNIQUE,\n",
        "        subject TEXT,\n",
        "        from_address TEXT,\n",
        "        date TEXT,\n",
        "        body TEXT,\n",
        "        file_name TEXT\n",
        "    );\n",
        "\n",
        "    CREATE TABLE recipients (\n",
        "        email_id TEXT,\n",
        "        recipient_address TEXT,\n",
        "        recipient_type TEXT\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    SQL_CREATE_INDEXES_TRIGGERS = \"\"\"\n",
        "    CREATE INDEX idx_emails_from ON emails(from_address);\n",
        "    CREATE INDEX idx_emails_date ON emails(date);\n",
        "    CREATE INDEX idx_emails_message_id ON emails(message_id);\n",
        "    CREATE INDEX idx_recipients_address ON recipients(recipient_address);\n",
        "    CREATE INDEX idx_recipients_type ON recipients(recipient_type);\n",
        "    CREATE INDEX idx_recipients_email_id ON recipients(email_id);\n",
        "    CREATE INDEX idx_recipients_address_email ON recipients(recipient_address, email_id);\n",
        "\n",
        "    CREATE VIRTUAL TABLE emails_fts USING fts5(\n",
        "        subject,\n",
        "        body,\n",
        "        content='emails',\n",
        "        content_rowid='id'\n",
        "    );\n",
        "\n",
        "    CREATE TRIGGER emails_ai AFTER INSERT ON emails BEGIN\n",
        "        INSERT INTO emails_fts (rowid, subject, body)\n",
        "        VALUES (new.id, new.subject, new.body);\n",
        "    END;\n",
        "\n",
        "    CREATE TRIGGER emails_ad AFTER DELETE ON emails BEGIN\n",
        "        DELETE FROM emails_fts WHERE rowid=old.id;\n",
        "    END;\n",
        "\n",
        "    CREATE TRIGGER emails_au AFTER UPDATE ON emails BEGIN\n",
        "        UPDATE emails_fts SET subject=new.subject, body=new.body WHERE rowid=old.id;\n",
        "    END;\n",
        "    \"\"\"\n",
        "\n",
        "    # Create database\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.executescript(SQL_CREATE_TABLES)\n",
        "    conn.commit()\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading full email dataset...\")\n",
        "    expected_features = Features(\n",
        "        {\n",
        "            \"message_id\": Value(\"string\"),\n",
        "            \"subject\": Value(\"string\"),\n",
        "            \"from\": Value(\"string\"),\n",
        "            \"to\": Sequence(Value(\"string\")),\n",
        "            \"cc\": Sequence(Value(\"string\")),\n",
        "            \"bcc\": Sequence(Value(\"string\")),\n",
        "            \"date\": Value(\"timestamp[us]\"),\n",
        "            \"body\": Value(\"string\"),\n",
        "            \"file_name\": Value(\"string\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    dataset = load_dataset(\n",
        "        EMAIL_DATASET_REPO_ID, features=expected_features, split=\"train\"\n",
        "    )\n",
        "    print(f\"Dataset contains {len(dataset)} total emails\")\n",
        "\n",
        "    # Populate database with ALL emails (not limited to 1000)\n",
        "    print(\"Populating database with all emails...\")\n",
        "    conn.execute(\"PRAGMA synchronous = OFF;\")\n",
        "    conn.execute(\"PRAGMA journal_mode = MEMORY;\")\n",
        "    conn.execute(\"BEGIN TRANSACTION;\")\n",
        "\n",
        "    record_count = 0\n",
        "    skipped_count = 0\n",
        "    duplicate_count = 0\n",
        "    processed_emails = set()  # Track (subject, body, from) tuples for deduplication\n",
        "\n",
        "    for email_data in tqdm(dataset, desc=\"Inserting emails\"):\n",
        "        message_id = email_data[\"message_id\"]\n",
        "        subject = email_data[\"subject\"]\n",
        "        from_address = email_data[\"from\"]\n",
        "        date_obj: datetime = email_data[\"date\"]\n",
        "        body = email_data[\"body\"]\n",
        "        file_name = email_data[\"file_name\"]\n",
        "        to_list = [str(addr) for addr in email_data[\"to\"] if addr]\n",
        "        cc_list = [str(addr) for addr in email_data[\"cc\"] if addr]\n",
        "        bcc_list = [str(addr) for addr in email_data[\"bcc\"] if addr]\n",
        "\n",
        "        # Apply the same filters as the original project\n",
        "        total_recipients = len(to_list) + len(cc_list) + len(bcc_list)\n",
        "\n",
        "        # Filter out very long emails and those with too many recipients\n",
        "        if len(body) > 5000:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        if total_recipients > 30:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # Deduplication check (same as original project)\n",
        "        email_key = (subject, body, from_address)\n",
        "        if email_key in processed_emails:\n",
        "            duplicate_count += 1\n",
        "            continue\n",
        "        else:\n",
        "            processed_emails.add(email_key)\n",
        "\n",
        "        date_str = date_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO emails (message_id, subject, from_address, date, body, file_name)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\",\n",
        "            (message_id, subject, from_address, date_str, body, file_name),\n",
        "        )\n",
        "\n",
        "        # Insert recipients\n",
        "        recipient_data = []\n",
        "        for addr in to_list:\n",
        "            recipient_data.append((message_id, addr, \"to\"))\n",
        "        for addr in cc_list:\n",
        "            recipient_data.append((message_id, addr, \"cc\"))\n",
        "        for addr in bcc_list:\n",
        "            recipient_data.append((message_id, addr, \"bcc\"))\n",
        "\n",
        "        if recipient_data:\n",
        "            cursor.executemany(\n",
        "                \"\"\"\n",
        "                INSERT INTO recipients (email_id, recipient_address, recipient_type)\n",
        "                VALUES (?, ?, ?)\n",
        "            \"\"\",\n",
        "                recipient_data,\n",
        "            )\n",
        "\n",
        "        record_count += 1\n",
        "\n",
        "    conn.commit()\n",
        "\n",
        "    # Create indexes and triggers\n",
        "    print(\"Creating indexes and FTS...\")\n",
        "    cursor.executescript(SQL_CREATE_INDEXES_TRIGGERS)\n",
        "    cursor.execute('INSERT INTO emails_fts(emails_fts) VALUES(\"rebuild\")')\n",
        "    conn.commit()\n",
        "\n",
        "    print(f\"Successfully created database with {record_count} emails.\")\n",
        "    print(f\"Skipped {skipped_count} emails due to length/recipient limits.\")\n",
        "    print(f\"Skipped {duplicate_count} duplicate emails.\")\n",
        "    return conn\n",
        "\n",
        "\n",
        "def get_db_connection():\n",
        "    \"\"\"Get database connection\"\"\"\n",
        "    if os.path.exists(DB_PATH):\n",
        "        print(f\"Loading existing database from {DB_PATH}\")\n",
        "        db_conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "    else:\n",
        "        db_conn = create_email_database()\n",
        "    return db_conn\n",
        "\n",
        "\n",
        "def search_emails(\n",
        "    inbox: str,\n",
        "    keywords: List[str],\n",
        "    from_addr: Optional[str] = None,\n",
        "    to_addr: Optional[str] = None,\n",
        "    sent_after: Optional[str] = None,\n",
        "    sent_before: Optional[str] = None,\n",
        "    max_results: int = 10,\n",
        ") -> List[SearchResult]:\n",
        "    \"\"\"Search the email database based on keywords and filters\"\"\"\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    where_clauses: List[str] = []\n",
        "    params: List[str | int] = []\n",
        "\n",
        "    if not keywords:\n",
        "        raise ValueError(\"No keywords provided for search.\")\n",
        "\n",
        "    if max_results > 10:\n",
        "        raise ValueError(\"max_results must be less than or equal to 10.\")\n",
        "\n",
        "    # FTS5 default is AND, so just join keywords. Escape quotes for safety.\n",
        "    fts_query = \" \".join(f\"\"\" \"{k.replace('\"', '\"\"')}\" \"\"\" for k in keywords)\n",
        "    where_clauses.append(\"fts.emails_fts MATCH ?\")\n",
        "    params.append(fts_query)\n",
        "\n",
        "    # Inbox filter\n",
        "    where_clauses.append(\"\"\"\n",
        "        (e.from_address = ? OR EXISTS (\n",
        "            SELECT 1 FROM recipients r_inbox\n",
        "            WHERE r_inbox.recipient_address = ? AND r_inbox.email_id = e.message_id\n",
        "        ))\n",
        "    \"\"\")\n",
        "    params.extend([inbox, inbox])\n",
        "\n",
        "    if from_addr:\n",
        "        where_clauses.append(\"e.from_address = ?\")\n",
        "        params.append(from_addr)\n",
        "\n",
        "    if to_addr:\n",
        "        where_clauses.append(\"\"\"\n",
        "            EXISTS (\n",
        "                SELECT 1 FROM recipients r_to\n",
        "                WHERE r_to.recipient_address = ? AND r_to.email_id = e.message_id\n",
        "            )\n",
        "        \"\"\")\n",
        "        params.append(to_addr)\n",
        "\n",
        "    if sent_after:\n",
        "        where_clauses.append(\"e.date >= ?\")\n",
        "        params.append(f\"{sent_after} 00:00:00\")\n",
        "\n",
        "    if sent_before:\n",
        "        where_clauses.append(\"e.date < ?\")\n",
        "        params.append(f\"{sent_before} 00:00:00\")\n",
        "\n",
        "    sql = f\"\"\"\n",
        "        SELECT\n",
        "            e.message_id,\n",
        "            snippet(emails_fts, -1, '<b>', '</b>', ' ... ', 15) as snippet\n",
        "        FROM\n",
        "            emails e JOIN emails_fts fts ON e.id = fts.rowid\n",
        "        WHERE\n",
        "            {\" AND \".join(where_clauses)}\n",
        "        ORDER BY\n",
        "            e.date DESC\n",
        "        LIMIT ?;\n",
        "    \"\"\"\n",
        "    params.append(max_results)\n",
        "\n",
        "    cursor.execute(sql, params)\n",
        "    results = cursor.fetchall()\n",
        "\n",
        "    return [SearchResult(message_id=row[0], snippet=row[1]) for row in results]\n",
        "\n",
        "\n",
        "def read_email(message_id: str) -> Optional[Email]:\n",
        "    \"\"\"Retrieve a single email by its message_id\"\"\"\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Get email details\n",
        "    cursor.execute(\n",
        "        \"SELECT message_id, date, subject, from_address, body, file_name FROM emails WHERE message_id = ?\",\n",
        "        (message_id,),\n",
        "    )\n",
        "    email_row = cursor.fetchone()\n",
        "\n",
        "    if not email_row:\n",
        "        return None\n",
        "\n",
        "    msg_id, date, subject, from_addr, body, file_name = email_row\n",
        "\n",
        "    # Get recipients\n",
        "    cursor.execute(\n",
        "        \"SELECT recipient_address, recipient_type FROM recipients WHERE email_id = ?\",\n",
        "        (message_id,),\n",
        "    )\n",
        "    recipient_rows = cursor.fetchall()\n",
        "\n",
        "    to_addresses = []\n",
        "    cc_addresses = []\n",
        "    bcc_addresses = []\n",
        "\n",
        "    for addr, type_val in recipient_rows:\n",
        "        if type_val.lower() == \"to\":\n",
        "            to_addresses.append(addr)\n",
        "        elif type_val.lower() == \"cc\":\n",
        "            cc_addresses.append(addr)\n",
        "        elif type_val.lower() == \"bcc\":\n",
        "            bcc_addresses.append(addr)\n",
        "\n",
        "    return Email(\n",
        "        message_id=msg_id,\n",
        "        date=date,\n",
        "        subject=subject,\n",
        "        from_address=from_addr,\n",
        "        to_addresses=to_addresses,\n",
        "        cc_addresses=cc_addresses,\n",
        "        bcc_addresses=bcc_addresses,\n",
        "        body=body,\n",
        "        file_name=file_name,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_training_scenarios(\n",
        "    split: Literal[\"train\", \"test\"] = \"train\",\n",
        "    limit: Optional[int] = None,\n",
        "    max_messages: Optional[int] = 1,\n",
        "    shuffle: bool = False,\n",
        "    seed: Optional[int] = None,\n",
        ") -> List[Scenario]:\n",
        "    \"\"\"Load training scenarios from Hugging Face dataset\"\"\"\n",
        "    print(f\"Loading {split} scenarios from Hugging Face...\")\n",
        "    dataset: Dataset = load_dataset(SCENARIO_DATASET_REPO_ID, split=split)\n",
        "\n",
        "    if max_messages is not None:\n",
        "        dataset = dataset.filter(lambda x: len(x[\"message_ids\"]) <= max_messages)\n",
        "\n",
        "    if shuffle or (seed is not None):\n",
        "        if seed is not None:\n",
        "            dataset = dataset.shuffle(seed=seed)\n",
        "        else:\n",
        "            dataset = dataset.shuffle()\n",
        "\n",
        "    # Convert each row to a Scenario object\n",
        "    scenarios = [Scenario(**row, split=split) for row in dataset]\n",
        "\n",
        "    if max_messages is not None:\n",
        "        scenarios = [s for s in scenarios if len(s.message_ids) <= max_messages]\n",
        "\n",
        "    if shuffle:\n",
        "        if seed is not None:\n",
        "            rng = random.Random(seed)\n",
        "            rng.shuffle(scenarios)\n",
        "        else:\n",
        "            random.shuffle(scenarios)\n",
        "\n",
        "    if limit is not None:\n",
        "        scenarios = scenarios[:limit]\n",
        "\n",
        "    print(f\"Loaded {len(scenarios)} scenarios.\")\n",
        "    return scenarios\n",
        "\n",
        "\n",
        "# Load training scenarios\n",
        "training_scenarios = load_training_scenarios(\n",
        "    split=\"train\", limit=50, max_messages=1, shuffle=True, seed=42\n",
        ")\n",
        "\n",
        "print(\"Email search environment created with full Enron dataset!\")\n",
        "print(\n",
        "    f\"Database contains the complete email dataset, loaded {len(training_scenarios)} training scenarios.\"\n",
        ")\n",
        "\n",
        "# print first scenario\n",
        "print(\"\\nSample scenario\")\n",
        "print(\"id:\", training_scenarios[0].id)\n",
        "print(\"question:\", training_scenarios[0].question)\n",
        "print(\"answer:\", training_scenarios[0].answer)\n",
        "print(\"message_ids:\", training_scenarios[0].message_ids)\n",
        "print(\"how_realistic:\", training_scenarios[0].how_realistic)\n",
        "print(\"inbox_address:\", training_scenarios[0].inbox_address)\n",
        "print(\"query_date:\", training_scenarios[0].query_date)\n",
        "print(\"split:\", training_scenarios[0].split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRi5EtmIX6I"
      },
      "source": [
        "### Creating a Model\n",
        "\n",
        "Now that we've defined the rules of our environment, we can create a model that will learn to search emails effectively. We'll use a Qwen 2.5 7B model for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSrU87EBIX6I"
      },
      "outputs": [],
      "source": [
        "import art\n",
        "from art.local import LocalBackend\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=\"email-agent-langgraph-001\",\n",
        "    project=\"email-search-agent-langgraph\",\n",
        "    base_model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=8192,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=0.8,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    # Normally we don't want to run the server in-process, but for the output\n",
        "    # to show up properly on Google Colab we'll enable this.\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend (sets up logging, inference, and training)\n",
        "await model.register(backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JImGVAwJIX6J"
      },
      "source": [
        "<a name=\"Rollout\"></a>\n",
        "\n",
        "### Defining a Rollout with LangGraph\n",
        "\n",
        "A rollout is a single episode of an agent performing its task. In this example, we'll use LangGraph's ReAct agent to handle the rollout. The rollout function presents the agent with an email search scenario, and the LangGraph agent uses the available tools to search for emails and answer the question.\n",
        "\n",
        "When the agent provides a final answer, the `correct` metric is calculated based on whether the answer is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txL32vtjIX6J",
        "outputId": "be12984c-d82b-4088-a246-083eeea55c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangGraph rollout function defined!\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "\n",
        "import weave\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from litellm import acompletion\n",
        "from tenacity import retry, stop_after_attempt\n",
        "from art.langgraph import init_chat_model\n",
        "\n",
        "import art\n",
        "\n",
        "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
        "    weave.init(model.project, settings={\"print_call_link\": False})\n",
        "\n",
        "MAX_TURNS = 20\n",
        "\n",
        "class CorrectnessJudgeResponse(BaseModel):\n",
        "    reasoning: str = Field(description=\"Explanation of the reasoning process.\")\n",
        "    accept: bool = Field(description=\"Whether the AI answer should be accepted.\")\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(3))\n",
        "async def judge_correctness(\n",
        "    scenario: Scenario, answer: str\n",
        ") -> CorrectnessJudgeResponse:\n",
        "    system_prompt = dedent(\n",
        "        \"\"\"\n",
        "        You are given a question, the reference answer (labelled **Reference answer**), and an answer generated by an AI assistant (labelled **AI answer**).\n",
        "\n",
        "        Your task is to decide whether the AI answer is correct and should be accepted. You should accept the answer if it contains the relevant information from the reference answer. You should not accept the answer if it is missing information relevant to the question, or if it contradicts the reference answer.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"Question: {scenario.question}\\n\"\n",
        "                f\"Reference answer: {scenario.answer}\\n\"\n",
        "                f\"AI answer: {answer}\"\n",
        "            ),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    response = await acompletion(\n",
        "        model=\"openai/gpt-4.1\",\n",
        "        messages=messages,\n",
        "        response_format=CorrectnessJudgeResponse,\n",
        "    )\n",
        "\n",
        "    first_choice = response.choices[0]\n",
        "    raw_content = first_choice.message.content or \"{}\"\n",
        "\n",
        "    try:\n",
        "        return CorrectnessJudgeResponse.model_validate_json(raw_content)\n",
        "    except Exception as e:\n",
        "        return CorrectnessJudgeResponse(\n",
        "            reasoning=f\"Parse error: {e}\\nRaw: {raw_content}\", accept=False\n",
        "        )\n",
        "\n",
        "\n",
        "class ProjectTrajectory(art.Trajectory):\n",
        "    final_answer: FinalAnswer | None = None\n",
        "\n",
        "\n",
        "class EmailScenario(BaseModel):\n",
        "    step: int\n",
        "    scenario: Scenario\n",
        "\n",
        "\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, email_scenario: EmailScenario) -> ProjectTrajectory:\n",
        "    scenario = email_scenario.scenario\n",
        "\n",
        "    traj = ProjectTrajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"scenario_id\": scenario.id,\n",
        "            \"step\": email_scenario.step,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    system_prompt = dedent(\n",
        "        f\"\"\"\n",
        "        You are an email search agent. You are given a user query and a list of tools you can use to search the user's email. Use the tools to search the user's emails and find the answer to the user's query. You may take up to {MAX_TURNS} turns to find the answer, so if your first search doesn't find the answer, you can try with different keywords.\n",
        "\n",
        "        User's email address is {scenario.inbox_address}\n",
        "        Today's date is {scenario.query_date}\n",
        "\n",
        "        When you have found the answer, use the return_final_answer_tool to provide your final answer along with the source message IDs.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Store final answer in trajectory\n",
        "    final_answer = None\n",
        "\n",
        "    # Define tools inside the rollout function to access local variables\n",
        "    @tool\n",
        "    def search_inbox_tool(keywords: list[str]) -> list[dict]:\n",
        "        \"\"\"Search the inbox for emails matching the given keywords and return\n",
        "        a list of dictionaries so the LLM can easily consume them.\"\"\"\n",
        "        results = search_emails(\n",
        "            inbox=scenario.inbox_address,\n",
        "            keywords=keywords,\n",
        "            sent_before=scenario.query_date,\n",
        "        )\n",
        "        return [asdict(result) for result in results]\n",
        "\n",
        "    @tool\n",
        "    def read_email_tool(message_id: str) -> dict | None:\n",
        "        \"\"\"Read a specific email by message ID.\"\"\"\n",
        "        email = read_email(message_id)\n",
        "        if email:\n",
        "            return email.model_dump()\n",
        "        return None\n",
        "\n",
        "    @tool\n",
        "    def return_final_answer_tool(answer: str, reference_message_ids: list[str]) -> dict:\n",
        "        \"\"\"Return the final answer and the message IDs of the emails that were used to generate the answer.\"\"\"\n",
        "        nonlocal final_answer\n",
        "        final_answer = FinalAnswer(answer=answer, source_ids=reference_message_ids)\n",
        "        return final_answer.model_dump()\n",
        "\n",
        "    # Create LangGraph tools\n",
        "    tools = [search_inbox_tool, read_email_tool, return_final_answer_tool]\n",
        "\n",
        "    chat_model = init_chat_model(model.name, temperature=1.0)\n",
        "\n",
        "    # Create the LangGraph ReAct agent\n",
        "    react_agent = create_react_agent(chat_model, tools)\n",
        "\n",
        "    try:\n",
        "        # Run the agent\n",
        "        config = {\n",
        "            \"configurable\": {\"thread_id\": str(uuid.uuid4())},\n",
        "            \"recursion_limit\": MAX_TURNS,\n",
        "        }\n",
        "\n",
        "        await react_agent.ainvoke(\n",
        "            {\n",
        "                \"messages\": [\n",
        "                    SystemMessage(content=system_prompt),\n",
        "                    HumanMessage(content=scenario.question),\n",
        "                ]\n",
        "            },\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        # Check if we got a final answer\n",
        "        if final_answer:\n",
        "            traj.final_answer = final_answer\n",
        "            # Score the trajectory\n",
        "            correctness_judge_response = await judge_correctness(\n",
        "                scenario, traj.final_answer.answer\n",
        "            )\n",
        "            traj.metrics[\"correct\"] = float(correctness_judge_response.accept)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error running LangGraph agent: {e}\")\n",
        "        # Add error information to trajectory\n",
        "        traj.messages_and_choices.append(\n",
        "            {\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"}\n",
        "        )\n",
        "\n",
        "    return traj\n",
        "\n",
        "\n",
        "print(\"LangGraph rollout function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqPKUCpeIX6J"
      },
      "source": [
        "<a name=\"ruler\"></a>\n",
        "\n",
        "### How RULER works\n",
        "\n",
        "**RULER** leverages two key insights:\n",
        "\n",
        "1. Relative scoring is easier than absolute scoring: It's easier for an LLM to rank several solutions relative to each other than to score them in isolation\n",
        "2. GRPO only needs relative scores: Since GRPO normalizes scores within each group, only the relative rankings matter, not absolute values\n",
        "\n",
        "The process:\n",
        "\n",
        "1. Generate N trajectories for a given scenario\n",
        "2. Pass all N trajectories to **RULER**\n",
        "3. **RULER** deduplicates common prefixes (e.g., identical system messages)\n",
        "4. An LLM judge scores each trajectory from 0 to 1 based on goal achievement\n",
        "5. These scores are used directly as rewards in GRPO training\n",
        "\n",
        "To learn more about **RULER**, check out the [RULER docs](https://art.openpipe.ai/fundamentals/ruler)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "0U1aFYfuIX6K",
        "outputId": "e55fcc07-5cd9-456e-d34c-c731f383bd88"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\">[</span>RULER<span style=\"font-weight: bold\">]</span> Pretty-printed LLM choice JSON:\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m[\u001b[0mRULER\u001b[1m]\u001b[0m Pretty-printed LLM choice JSON:\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'scores'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Counts correctly to 10 using numeric symbols as instructed.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Counts to 10 correctly but uses words instead of numeric symbols, not following the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">system instruction.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'trajectory_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'explanation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Does not count numbers at all, uses letters instead of numerals.'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'scores'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Counts correctly to 10 using numeric symbols as instructed.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Counts to 10 correctly but uses words instead of numeric symbols, not following the \u001b[0m\n",
              "\u001b[32msystem instruction.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.5\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'trajectory_id'\u001b[0m: \u001b[32m'3'\u001b[0m,\n",
              "            \u001b[32m'explanation'\u001b[0m: \u001b[32m'Does not count numbers at all, uses letters instead of numerals.'\u001b[0m,\n",
              "            \u001b[32m'score'\u001b[0m: \u001b[1;36m0.0\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Rank 1: Score 1.000\n",
            "  Response: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10...\n",
            "\n",
            "Rank 2: Score 0.500\n",
            "  Response: one, two, three, four, five, six, seven, eight, ni...\n",
            "\n",
            "Rank 3: Score 0.000\n",
            "  Response: a, b, c, d, e, f, g, h, i, j...\n"
          ]
        }
      ],
      "source": [
        "#@title Sample RULER evaluation\n",
        "\n",
        "import art\n",
        "from art.rewards import ruler_score_group\n",
        "\n",
        "# Test RULER with a simple example\n",
        "base_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You count numbers using numeric symbols.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Count to 10.\"},\n",
        "]\n",
        "\n",
        "good_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "mediocre_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"one, two, three, four, five, six, seven, eight, nine, ten\",\n",
        "        },\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "bad_trajectory = art.Trajectory(\n",
        "    messages_and_choices=[\n",
        "        *base_messages,\n",
        "        {\"role\": \"assistant\", \"content\": \"a, b, c, d, e, f, g, h, i, j\"},\n",
        "    ],\n",
        "    reward=0,\n",
        ")\n",
        "\n",
        "sample_group = art.TrajectoryGroup(\n",
        "    trajectories=[\n",
        "        good_trajectory,\n",
        "        mediocre_trajectory,\n",
        "        bad_trajectory,\n",
        "    ]\n",
        ")\n",
        "\n",
        "judged_group = await ruler_score_group(sample_group, \"openai/o4-mini\", debug=True)\n",
        "assert judged_group is not None\n",
        "\n",
        "# Display rankings\n",
        "sorted_trajectories = sorted(\n",
        "    judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        ")\n",
        "for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "    messages = traj.messages()\n",
        "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "    print(f\"  Response: {messages[-1]['content'][:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEC3r48iIX6K"
      },
      "source": [
        "<a name=\"Loop\"></a>\n",
        "\n",
        "### Training Loop with LangGraph\n",
        "\n",
        "The training loop is where the magic happens. For each of the steps defined below, the rollout function will be called multiple times in parallel using LangGraph's ReAct agent. Each scenario will produce a trajectory, which will be used to update the model.\n",
        "\n",
        "The `gather` step will wait for all of the trajectories to be generated, then it will use RULER to assign relative scores to each trajectory.\n",
        "\n",
        "Our notebook will then delete all but the most recent checkpoint and train the model on the scored trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLMkONqFIX6K"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "from art.utils import iterate_dataset\n",
        "from art.langgraph import wrap_rollout\n",
        "\n",
        "training_config = {\n",
        "    \"groups_per_step\": 2,\n",
        "    \"num_epochs\": 20,\n",
        "    \"rollouts_per_group\": 4,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"max_steps\": 20,\n",
        "}\n",
        "\n",
        "# Use iterate_dataset with real training scenarios (similar to train.py)\n",
        "training_iterator = iterate_dataset(\n",
        "    training_scenarios,  # Use real scenarios from Hugging Face\n",
        "    groups_per_step=training_config[\"groups_per_step\"],\n",
        "    num_epochs=training_config[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "\n",
        "for batch in training_iterator:\n",
        "    print(\n",
        "        f\"Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step}\"\n",
        "    )\n",
        "    print(f\"Batch contains {len(batch.items)} scenarios\")\n",
        "\n",
        "    # Create trajectory groups for this batch (similar to train.py)\n",
        "    groups = []\n",
        "    for scenario in batch.items:\n",
        "        groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    wrap_rollout(model, rollout)(\n",
        "                        model, EmailScenario(step=batch.step, scenario=scenario)\n",
        "                    )\n",
        "                    for _ in range(training_config[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    print(groups[0])\n",
        "    # Gather all trajectory groups\n",
        "    finished_groups = await art.gather_trajectory_groups(\n",
        "        groups,\n",
        "        pbar_desc=\"gather\",\n",
        "        max_exceptions=training_config[\"rollouts_per_group\"] * len(batch.items),\n",
        "    )\n",
        "\n",
        "    judged_groups = []\n",
        "    for group in finished_groups:\n",
        "        # Use RULER to assign relative scores to each trajectory\n",
        "        judged_group = await ruler_score_group(group, \"openai/o4-mini\", debug=True)\n",
        "        judged_groups.append(judged_group)\n",
        "\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        judged_groups,\n",
        "        config=art.TrainConfig(learning_rate=training_config[\"learning_rate\"]),\n",
        "        # Lowering the logprob_calculation_chunk_size is a memory saving measure\n",
        "        # to allow longer sequences (up to 8192 tokens) to be processed on a T4.\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )\n",
        "\n",
        "    print(f\"Completed training step {batch.step}\")\n",
        "\n",
        "    # Stop after max_steps for demo purposes (adjust as needed)\n",
        "    if batch.step >= training_config[\"max_steps\"]:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wnlNGMlIX6K"
      },
      "source": [
        "### Using the Model\n",
        "\n",
        "Just like that, you've trained an agent to search emails and answer questions using LangGraph! Now it's time to use your model outside of the training loop.\n",
        "\n",
        "Check out the code below for a small demo of the model you just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xvofjjLmIX6L"
      },
      "outputs": [],
      "source": [
        "#@title Loading/inference code\n",
        "\n",
        "# Test the trained model using the rollout function\n",
        "# This avoids memory issues and uses the same inference path as training\n",
        "\n",
        "print(\"Testing the trained LangGraph model with a real scenario...\\n\")\n",
        "\n",
        "\n",
        "# Use a scenario from our training set\n",
        "test_scenario = training_scenarios[1]\n",
        "\n",
        "print(f\"Test scenario ID: {test_scenario.id}\")\n",
        "print(f\"Question: {test_scenario.question}\")\n",
        "print(f\"Expected answer: {test_scenario.answer}\")\n",
        "print(f\"Reference message IDs: {test_scenario.message_ids}\")\n",
        "print(f\"Inbox: {test_scenario.inbox_address}\")\n",
        "print(f\"Query date: {test_scenario.query_date}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Run the rollout function with the trained model\n",
        "test_email_scenario = EmailScenario.model_validate(\n",
        "    {\"step\": 0, \"scenario\": test_scenario.model_dump()}\n",
        ")\n",
        "result_trajectory = await wrap_rollout(model, rollout)(model, test_email_scenario)\n",
        "\n",
        "print(\"LangGraph Agent's trajectory:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Display the conversation\n",
        "messages = result_trajectory.messages()\n",
        "for i, msg in enumerate(messages):\n",
        "    role = msg.get(\"role\", \"unknown\")\n",
        "    content = msg.get(\"content\", \"\")\n",
        "    tool_calls = msg.get(\"tool_calls\", [])\n",
        "\n",
        "    if role == \"system\":\n",
        "        print(\n",
        "            f\"[SYSTEM]: {content[:100]}...\"\n",
        "            if len(content) > 100\n",
        "            else f\"[SYSTEM]: {content}\"\n",
        "        )\n",
        "    elif role == \"user\":\n",
        "        print(f\"[USER]: {content}\")\n",
        "    elif role == \"assistant\":\n",
        "        if tool_calls:\n",
        "            print(f\"[ASSISTANT]: {tool_calls}\")\n",
        "        if content:\n",
        "            print(f\"[ASSISTANT]: {content}\")\n",
        "    elif role == \"tool\":\n",
        "        tool_name = msg.get(\"name\", \"unknown_tool\")\n",
        "        print(\n",
        "            f\"[TOOL - {tool_name}]: {content[:200]}...\"\n",
        "            if len(content) > 200\n",
        "            else f\"[TOOL - {tool_name}]: {content}\"\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "if result_trajectory.final_answer:\n",
        "    print(f\"Agent's Final Answer: {result_trajectory.final_answer.answer}\")\n",
        "    print(f\"Source IDs Used: {result_trajectory.final_answer.source_ids}\")\n",
        "else:\n",
        "    print(\"No final answer provided by the agent\")\n",
        "\n",
        "print(f\"\\nExpected Answer: {test_scenario.answer}\")\n",
        "print(f\"Expected Source IDs: {test_scenario.message_ids}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ LangGraph email search agent testing completed!\")\n",
        "print(\n",
        "    \"The agent used LangGraph's ReAct pattern with the same inference path as during training.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omIgMH2KIX6L"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2e54d5b"
      },
      "source": [
        "# Task\n",
        "Explain how to set up an RL task to write and reply to emails based on user criteria and examples of their writing style."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59264a5a"
      },
      "source": [
        "## Define the email environment\n",
        "\n",
        "### Subtask:\n",
        "Create a simulated environment where the agent receives emails and needs to compose replies or new emails. This would involve defining the state of the environment (e.g., the incoming email, the user's instructions/criteria).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18537ede"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the data structures for the email environment state, including incoming email details, user instructions, and a place for writing style examples. This addresses steps 1, 2, 3, and 4 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14bedc7e",
        "outputId": "ac9d4991-8d34-49ff-8ada-a5c66f9dc2b3"
      },
      "source": [
        "from typing import List, Optional, Dict, Any\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Data structure to represent an email\n",
        "class SimulatedEmail(BaseModel):\n",
        "    \"\"\"Represents a simulated email.\"\"\"\n",
        "    message_id: str = Field(description=\"Unique identifier for the email.\")\n",
        "    date: str = Field(description=\"Date the email was sent (ISO 8601 format).\")\n",
        "    subject: Optional[str] = Field(None, description=\"Subject line of the email.\")\n",
        "    from_address: str = Field(description=\"Sender's email address.\")\n",
        "    to_addresses: List[str] = Field([], description=\"List of 'To' recipient email addresses.\")\n",
        "    cc_addresses: List[str] = Field([], description=\"List of 'CC' recipient email addresses.\")\n",
        "    bcc_addresses: List[str] = Field([], description=\"List of 'BCC' recipient email addresses.\")\n",
        "    body: str = Field(description=\"Body content of the email.\")\n",
        "    attachments: List[str] = Field([], description=\"List of attachment names/identifiers.\")\n",
        "\n",
        "# Data structure to represent user instructions\n",
        "class UserInstructions(BaseModel):\n",
        "    \"\"\"Represents user instructions for composing an email.\"\"\"\n",
        "    task_type: str = Field(description=\"Type of task: 'reply' or 'new_email'.\")\n",
        "    recipient: Optional[str | List[str]] = Field(None, description=\"Recipient(s) for a new email.\")\n",
        "    subject: Optional[str] = Field(None, description=\"Subject for a new email.\")\n",
        "    key_points: List[str] = Field([], description=\"Key points to include in the email.\")\n",
        "    tone: Optional[str] = Field(None, description=\"Desired tone for the email (e.g., 'formal', 'casual').\")\n",
        "    length: Optional[str] = Field(None, description=\"Desired length (e.g., 'brief', 'detailed').\")\n",
        "    other_criteria: Optional[Dict[str, Any]] = Field(None, description=\"Any other specific instructions.\")\n",
        "\n",
        "# Data structure to represent the state of the email environment for the agent\n",
        "class EmailEnvironmentState(BaseModel):\n",
        "    \"\"\"Represents the state of the email environment for the agent.\"\"\"\n",
        "    incoming_email: Optional[SimulatedEmail] = Field(None, description=\"The incoming email if the task is a reply.\")\n",
        "    user_instructions: UserInstructions = Field(description=\"Instructions from the user for the email task.\")\n",
        "    writing_style_examples: List[SimulatedEmail] = Field([], description=\"Examples of the user's writing style.\")\n",
        "    current_date: str = Field(description=\"The current date in the simulated environment (ISO 8601 format).\")\n",
        "\n",
        "print(\"Defined data structures for the email environment state.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined data structures for the email environment state.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94f81e92"
      },
      "source": [
        "## Develop tools/actions\n",
        "\n",
        "### Subtask:\n",
        "Define the actions the agent can take within the environment. This might include tools for drafting email content, sending the email, or perhaps even searching for information to include in the email.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd0ad779"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the actions as functions or classes that can be called by the agent, including drafting email content, sending the email, and searching for information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960382d8",
        "outputId": "49fcae00-891e-4f29-e023-49c952dadf55"
      },
      "source": [
        "from typing import List, Optional, Dict, Any\n",
        "from pydantic import BaseModel, Field\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# Data structure to represent drafted email components\n",
        "class DraftedEmail(BaseModel):\n",
        "    \"\"\"Represents components of a drafted email.\"\"\"\n",
        "    to_addresses: List[str] = Field([], description=\"List of 'To' recipient email addresses.\")\n",
        "    cc_addresses: List[str] = Field([], description=\"List of 'CC' recipient email addresses.\")\n",
        "    bcc_addresses: List[str] = Field([], description=\"List of 'BCC' recipient email addresses.\")\n",
        "    subject: str = Field(\"\", description=\"Subject line of the email.\")\n",
        "    body: str = Field(\"\", description=\"Body content of the email.\")\n",
        "\n",
        "# Action 1: Draft email content\n",
        "def draft_email_action(state: EmailEnvironmentState, draft_components: DraftedEmail) -> DraftedEmail:\n",
        "    \"\"\"\n",
        "    Simulates the agent drafting email content.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the email environment.\n",
        "        draft_components: The current draft components being built by the agent.\n",
        "\n",
        "    Returns:\n",
        "        The updated drafted email components.\n",
        "    \"\"\"\n",
        "    # In a real RL setup, the agent would generate draft_components based on state\n",
        "    # This is a placeholder function that just returns the provided components\n",
        "    print(\"Agent is drafting email content...\")\n",
        "    return draft_components\n",
        "\n",
        "# Action 2: Send the email\n",
        "def send_email_action(drafted_email: DraftedEmail) -> SimulatedEmail:\n",
        "    \"\"\"\n",
        "    Simulates sending the drafted email.\n",
        "\n",
        "    Args:\n",
        "        drafted_email: The drafted email components.\n",
        "\n",
        "    Returns:\n",
        "        A SimulatedEmail object representing the sent email.\n",
        "    \"\"\"\n",
        "    print(\"Agent is sending the email...\")\n",
        "    # Simulate creating a sent email object\n",
        "    sent_email = SimulatedEmail(\n",
        "        message_id=str(uuid.uuid4()),\n",
        "        date=datetime.now().isoformat(), # Use current simulation time\n",
        "        subject=drafted_email.subject,\n",
        "        from_address=\"simulated_user@example.com\", # Assume a fixed user address for simulation\n",
        "        to_addresses=drafted_email.to_addresses,\n",
        "        cc_addresses=drafted_email.cc_addresses,\n",
        "        bcc_addresses=drafted_email.bcc_addresses,\n",
        "        body=drafted_email.body,\n",
        "        attachments=[] # For simplicity, no attachments in this simulation\n",
        "    )\n",
        "    print(f\"Email sent with Subject: {sent_email.subject}\")\n",
        "    return sent_email\n",
        "\n",
        "# Action 3: Search for information (Placeholder)\n",
        "def search_information_action(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulates searching for information based on a query.\n",
        "\n",
        "    Args:\n",
        "        query: The search query from the agent.\n",
        "\n",
        "    Returns:\n",
        "        A string containing simulated search results.\n",
        "    \"\"\"\n",
        "    print(f\"Agent is searching for information with query: '{query}'\")\n",
        "    # In a real RL setup, this would interact with a knowledge base or search API\n",
        "    # This is a placeholder\n",
        "    simulated_results = f\"Simulated search results for '{query}': Information found regarding the query.\"\n",
        "    print(f\"Simulated search results: {simulated_results[:50]}...\")\n",
        "    return simulated_results\n",
        "\n",
        "print(\"Defined actions for the email agent.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined actions for the email agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09bc173f"
      },
      "source": [
        "## Create training data\n",
        "\n",
        "### Subtask:\n",
        "Gather a dataset of emails you have written or replied to, along with the context (the email you were replying to, or the situation for a new email) and your intended message or style.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b363fbda"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires gathering a dataset of emails with context and extracting key information. Since I cannot access external files or user data directly, I will simulate the creation of a small, sample dataset in the required structured format based on the previously defined `SimulatedEmail` and `UserInstructions` models. This will demonstrate the structure of the dataset needed for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7266bdd5",
        "outputId": "1a50814d-26e4-439d-ce1b-dc808daaf94e"
      },
      "source": [
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "# Simulate creating a small sample dataset\n",
        "# This dataset will contain examples of email scenarios, user instructions,\n",
        "# and the corresponding desired email output (simulated from \"past emails\").\n",
        "# In a real scenario, this data would be gathered from a user's actual emails.\n",
        "\n",
        "sample_training_dataset = []\n",
        "\n",
        "# Example 1: Replying to an email\n",
        "incoming_email_1 = SimulatedEmail(\n",
        "    message_id=str(uuid.uuid4()),\n",
        "    date=\"2023-10-26T10:00:00\",\n",
        "    subject=\"Meeting Tomorrow\",\n",
        "    from_address=\"colleague@example.com\",\n",
        "    to_addresses=[\"simulated_user@example.com\"],\n",
        "    body=\"Hi, just a reminder about our meeting tomorrow at 2 PM. Can you confirm if you'll be there?\",\n",
        ")\n",
        "user_instructions_1 = UserInstructions(\n",
        "    task_type=\"reply\",\n",
        "    key_points=[\"Confirm attendance\", \"Looking forward to it\"],\n",
        "    tone=\"friendly\",\n",
        "    length=\"brief\",\n",
        ")\n",
        "# Simulated \"past email\" (the desired reply)\n",
        "desired_output_email_1 = SimulatedEmail(\n",
        "    message_id=str(uuid.uuid4()), # This would be the ID of the email the user actually sent\n",
        "    date=\"2023-10-26T11:00:00\",\n",
        "    subject=\"Re: Meeting Tomorrow\",\n",
        "    from_address=\"simulated_user@example.com\",\n",
        "    to_addresses=[\"colleague@example.com\"],\n",
        "    body=\"Hi, Yes, I'll be there. Looking forward to it!\",\n",
        ")\n",
        "\n",
        "sample_training_dataset.append({\n",
        "    \"context\": {\n",
        "        \"incoming_email\": incoming_email_1.model_dump(),\n",
        "        \"user_instructions\": user_instructions_1.model_dump(),\n",
        "        \"writing_style_examples\": [], # Add relevant style examples here if available\n",
        "        \"current_date\": \"2023-10-26T11:00:00\"\n",
        "    },\n",
        "    \"desired_output\": desired_output_email_1.model_dump()\n",
        "})\n",
        "\n",
        "# Example 2: Composing a new email\n",
        "user_instructions_2 = UserInstructions(\n",
        "    task_type=\"new_email\",\n",
        "    recipient=\"manager@example.com\",\n",
        "    subject=\"Project Update\",\n",
        "    key_points=[\"Project Alpha on track\", \"Milestone X completed\", \"Need approval for Phase 2\"],\n",
        "    tone=\"formal\",\n",
        "    length=\"detailed\",\n",
        "    other_criteria={\"attachments_needed\": False}\n",
        ")\n",
        "# Simulated \"past email\" (the desired new email)\n",
        "desired_output_email_2 = SimulatedEmail(\n",
        "    message_id=str(uuid.uuid4()),\n",
        "    date=\"2023-10-27T09:00:00\",\n",
        "    subject=\"Project Update - Alpha\",\n",
        "    from_address=\"simulated_user@example.com\",\n",
        "    to_addresses=[\"manager@example.com\"],\n",
        "    body=\"Dear Manager, Please find below an update on Project Alpha. The project remains on track. We have successfully completed Milestone X. We require your approval to proceed with Phase 2. Thank you.\",\n",
        ")\n",
        "\n",
        "sample_training_dataset.append({\n",
        "    \"context\": {\n",
        "        \"incoming_email\": None, # No incoming email for a new email task\n",
        "        \"user_instructions\": user_instructions_2.model_dump(),\n",
        "        \"writing_style_examples\": [], # Add relevant style examples here if available\n",
        "        \"current_date\": \"2023-10-27T09:00:00\"\n",
        "    },\n",
        "    \"desired_output\": desired_output_email_2.model_dump()\n",
        "})\n",
        "\n",
        "# Display the structure of the sample dataset\n",
        "print(f\"Created a sample training dataset with {len(sample_training_dataset)} entries.\")\n",
        "import json\n",
        "print(\"Sample dataset structure:\")\n",
        "print(json.dumps(sample_training_dataset[0], indent=2))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a sample training dataset with 2 entries.\n",
            "Sample dataset structure:\n",
            "{\n",
            "  \"context\": {\n",
            "    \"incoming_email\": {\n",
            "      \"message_id\": \"88d7ab39-4660-4654-9d09-fc7d68c4082f\",\n",
            "      \"date\": \"2023-10-26T10:00:00\",\n",
            "      \"subject\": \"Meeting Tomorrow\",\n",
            "      \"from_address\": \"colleague@example.com\",\n",
            "      \"to_addresses\": [\n",
            "        \"simulated_user@example.com\"\n",
            "      ],\n",
            "      \"cc_addresses\": [],\n",
            "      \"bcc_addresses\": [],\n",
            "      \"body\": \"Hi, just a reminder about our meeting tomorrow at 2 PM. Can you confirm if you'll be there?\",\n",
            "      \"attachments\": []\n",
            "    },\n",
            "    \"user_instructions\": {\n",
            "      \"task_type\": \"reply\",\n",
            "      \"recipient\": null,\n",
            "      \"subject\": null,\n",
            "      \"key_points\": [\n",
            "        \"Confirm attendance\",\n",
            "        \"Looking forward to it\"\n",
            "      ],\n",
            "      \"tone\": \"friendly\",\n",
            "      \"length\": \"brief\",\n",
            "      \"other_criteria\": null\n",
            "    },\n",
            "    \"writing_style_examples\": [],\n",
            "    \"current_date\": \"2023-10-26T11:00:00\"\n",
            "  },\n",
            "  \"desired_output\": {\n",
            "    \"message_id\": \"a5da1965-1035-49eb-9ad1-75446ff62e22\",\n",
            "    \"date\": \"2023-10-26T11:00:00\",\n",
            "    \"subject\": \"Re: Meeting Tomorrow\",\n",
            "    \"from_address\": \"simulated_user@example.com\",\n",
            "    \"to_addresses\": [\n",
            "      \"colleague@example.com\"\n",
            "    ],\n",
            "    \"cc_addresses\": [],\n",
            "    \"bcc_addresses\": [],\n",
            "    \"body\": \"Hi, Yes, I'll be there. Looking forward to it!\",\n",
            "    \"attachments\": []\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e474fa2"
      },
      "source": [
        "## Design the reward function\n",
        "\n",
        "### Subtask:\n",
        "Design the reward function. This is a crucial step. You need a way to automatically evaluate how good the agent's generated emails are. This could involve:\n",
        "\n",
        "*   Using an LLM judge (similar to RULER) to score emails based on your criteria (e.g., tone, clarity, inclusion of key points).\n",
        "*   Potentially comparing generated emails to your examples or gold-standard replies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a6c72cf"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `evaluate_email` function as requested, incorporating the use of an LLM judge (conceptually similar to RULER) and a comparison to the desired output email from the training data to generate a reward signal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c2c13de",
        "outputId": "59b91116-42a4-4b75-ae2b-eb34c973f6f8"
      },
      "source": [
        "from typing import List, Optional, Dict, Any\n",
        "from pydantic import BaseModel, Field\n",
        "import json\n",
        "import random # Used for simulating LLM judge variability\n",
        "\n",
        "# Assuming SimulatedEmail, UserInstructions, and EmailEnvironmentState are defined in previous cells\n",
        "\n",
        "class LLMJudgeScore(BaseModel):\n",
        "    \"\"\"Represents the output structure from a simulated LLM judge.\"\"\"\n",
        "    reasoning: str = Field(description=\"Explanation from the LLM judge.\")\n",
        "    score: float = Field(description=\"A numerical score from 0 to 1 based on evaluation criteria.\")\n",
        "\n",
        "# This is a simulated LLM judge function. In a real implementation, this would\n",
        "# involve calling an actual LLM API with a carefully crafted prompt.\n",
        "async def simulate_llm_judge(\n",
        "    generated_email: SimulatedEmail,\n",
        "    environment_state: EmailEnvironmentState,\n",
        "    desired_output_email: Optional[SimulatedEmail] = None # Included for comparison\n",
        ") -> LLMJudgeScore:\n",
        "    \"\"\"\n",
        "    Simulates an LLM judging the quality of a generated email.\n",
        "\n",
        "    Args:\n",
        "        generated_email: The email generated by the agent.\n",
        "        environment_state: The context in which the email was generated.\n",
        "        desired_output_email: The gold-standard or desired email for comparison.\n",
        "\n",
        "    Returns:\n",
        "        A simulated LLMJudgeScore object.\n",
        "    \"\"\"\n",
        "    print(\"Simulating LLM judge evaluation...\")\n",
        "\n",
        "    # Access user instructions and context from the environment state\n",
        "    instructions = environment_state.user_instructions\n",
        "    incoming_email = environment_state.incoming_email\n",
        "    writing_style_examples = environment_state.writing_style_examples\n",
        "\n",
        "    # --- LLM Judge Criteria Outline ---\n",
        "    # The actual prompt for the LLM judge would detail these points:\n",
        "    # 1. Adherence to UserInstructions:\n",
        "    #    - Inclusion of all key_points?\n",
        "    #    - Correct tone (formal, casual, etc.)?\n",
        "    #    - Appropriate length (brief, detailed)?\n",
        "    #    - Any other_criteria met?\n",
        "    # 2. Contextual Appropriateness:\n",
        "    #    - If a reply, does it logically follow the incoming_email?\n",
        "    #    - Is it appropriate for the recipient(s)?\n",
        "    # 3. Writing Style:\n",
        "    #    - Does it match the style of writing_style_examples (if provided)? (This is harder for a simple judge, might need specific few-shot examples or fine-tuning).\n",
        "    # 4. Overall Quality:\n",
        "    #    - Clarity, coherence, grammar, spelling.\n",
        "\n",
        "    # --- Scoring Logic (Simulated) ---\n",
        "    # This is a simplified simulation. A real judge would use the LLM's output\n",
        "    # to derive a structured score or ranking.\n",
        "\n",
        "    score = 0.0\n",
        "    reasoning_points = []\n",
        "\n",
        "    # Simulate scoring based on instructions\n",
        "    if all(point in generated_email.body for point in instructions.key_points):\n",
        "        score += 0.4 # Reward for including key points\n",
        "        reasoning_points.append(\"Included all key points.\")\n",
        "    else:\n",
        "        reasoning_points.append(\"Missed some key points.\")\n",
        "\n",
        "    # Simple tone check simulation (very basic)\n",
        "    if instructions.tone == \"formal\" and (\"Dear\" in generated_email.body and \"Sincerely\" in generated_email.body):\n",
        "         score += 0.2\n",
        "         reasoning_points.append(\"Tone appears formal.\")\n",
        "    elif instructions.tone == \"friendly\" and (\"Hi,\" in generated_email.body or \"Thanks,\" in generated_email.body):\n",
        "         score += 0.2\n",
        "         reasoning_points.append(\"Tone appears friendly.\")\n",
        "    else:\n",
        "         reasoning_points.append(f\"Tone evaluation based on criteria '{instructions.tone}' inconclusive.\")\n",
        "\n",
        "\n",
        "    # Simulate length check (very basic)\n",
        "    generated_length = len(generated_email.body.split())\n",
        "    if instructions.length == \"brief\" and generated_length < 100:\n",
        "        score += 0.1\n",
        "        reasoning_points.append(\"Email is brief as requested.\")\n",
        "    elif instructions.length == \"detailed\" and generated_length > 100:\n",
        "        score += 0.1\n",
        "        reasoning_points.append(\"Email is detailed as requested.\")\n",
        "    else:\n",
        "        reasoning_points.append(f\"Length evaluation based on criteria '{instructions.length}' inconclusive.\")\n",
        "\n",
        "\n",
        "    # --- Comparison to Desired Output (if available) ---\n",
        "    # This is a crucial part for supervised-like signals.\n",
        "    if desired_output_email:\n",
        "        # Simulate comparison - a real implementation could use:\n",
        "        # - String matching (exact or fuzzy)\n",
        "        # - Semantic similarity (e.g., using embeddings)\n",
        "        # - Overlap in key information extracted from both emails\n",
        "\n",
        "        # Simple simulation: reward if the body is an exact match (unlikely in reality)\n",
        "        # Or, a more realistic approach: reward based on similarity metrics.\n",
        "        # For this simulation, let's use a simplified overlap check.\n",
        "        desired_body_words = set(desired_output_email.body.lower().split())\n",
        "        generated_body_words = set(generated_email.body.lower().split())\n",
        "        common_words = desired_body_words.intersection(generated_body_words)\n",
        "        overlap_ratio = len(common_words) / max(len(desired_body_words), 1) # Avoid division by zero\n",
        "\n",
        "        # Reward based on word overlap (simple proxy for content similarity)\n",
        "        score += overlap_ratio * 0.3 # Allocate some reward for content similarity\n",
        "\n",
        "        if overlap_ratio > 0.5:\n",
        "             reasoning_points.append(f\"Generated email has significant word overlap with desired output ({overlap_ratio:.2f}).\")\n",
        "        else:\n",
        "             reasoning_points.append(f\"Generated email has low word overlap with desired output ({overlap_ratio:.2f}).\")\n",
        "\n",
        "    # Add a small random component to simulate LLM variability and exploration\n",
        "    score = max(0.0, min(1.0, score + random.uniform(-0.05, 0.05))) # Ensure score is between 0 and 1\n",
        "\n",
        "    reasoning = \" \".join(reasoning_points) if reasoning_points else \"Basic evaluation performed.\"\n",
        "\n",
        "    return LLMJudgeScore(reasoning=reasoning, score=score)\n",
        "\n",
        "\n",
        "# Define the main reward function\n",
        "async def evaluate_email(\n",
        "    generated_email: SimulatedEmail,\n",
        "    environment_state: EmailEnvironmentState,\n",
        "    desired_output_email: Optional[SimulatedEmail] = None # From training data\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evaluates the quality of a generated email and returns a reward score.\n",
        "\n",
        "    This function combines the assessment from a simulated LLM judge\n",
        "    and a comparison to the desired output email (if provided in training data).\n",
        "\n",
        "    Args:\n",
        "        generated_email: The email generated by the agent.\n",
        "        environment_state: The context in which the email was generated.\n",
        "        desired_output_email: The gold-standard or desired email from the training data.\n",
        "\n",
        "    Returns:\n",
        "        A numerical reward score for the generated email.\n",
        "    \"\"\"\n",
        "    print(\"Evaluating generated email...\")\n",
        "\n",
        "    # Use the simulated LLM judge to get an initial score based on instructions and context\n",
        "    judge_result = await simulate_llm_judge(generated_email, environment_state, desired_output_email)\n",
        "\n",
        "    # The LLM judge score is a primary component of the reward\n",
        "    reward_score = judge_result.score\n",
        "\n",
        "    # Additional potential reward/penalty logic could go here, for example:\n",
        "    # - Penalize for errors (e.g., sending to wrong recipient, empty body)\n",
        "    # - Reward for efficiency (e.g., using minimal steps if applicable)\n",
        "\n",
        "    # For this design, the LLM judge score (which already incorporates comparison\n",
        "    # to desired output via simulation) is the main reward signal.\n",
        "\n",
        "    print(f\"Evaluation complete. Reward score: {reward_score:.4f}\")\n",
        "    print(f\"Judge Reasoning: {judge_result.reasoning}\")\n",
        "\n",
        "    return reward_score\n",
        "\n",
        "print(\"Defined the evaluate_email function for reward calculation.\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined the evaluate_email function for reward calculation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e45e124"
      },
      "source": [
        "## Adapt the agent architecture\n",
        "\n",
        "### Subtask:\n",
        "Adapt the agent architecture. Modify or create a LangGraph agent (or similar) that can interact with your custom environment and use the defined tools to generate email content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed2dfabe"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the LangGraph state and workflow, incorporating the previously defined actions as nodes and handling transitions based on the agent's process of drafting and sending an email.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "731f6f94",
        "outputId": "06a92489-196c-41ce-f2d1-c7f31b9fa1ae"
      },
      "source": [
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Assuming SimulatedEmail, UserInstructions, EmailEnvironmentState,\n",
        "# draft_email_action, send_email_action, and search_information_action\n",
        "# are defined in previous cells.\n",
        "\n",
        "# 1. Define the state graph\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of the LangGraph agent.\n",
        "\n",
        "    Attributes:\n",
        "        environment_state: The current state of the email environment.\n",
        "        drafted_email: The email being drafted by the agent.\n",
        "        messages: A list of messages representing the conversation history (for agent reasoning).\n",
        "        search_results: Results from information search (if performed).\n",
        "        final_email_sent: Flag indicating if the email has been sent.\n",
        "    \"\"\"\n",
        "    environment_state: EmailEnvironmentState\n",
        "    drafted_email: DraftedEmail\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    search_results: Optional[str]\n",
        "    final_email_sent: bool\n",
        "\n",
        "\n",
        "# 2. Implement the LangGraph workflow\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes for the graph\n",
        "def call_draft_email(state: AgentState) -> AgentState:\n",
        "    \"\"\"Calls the draft_email_action and updates the state.\"\"\"\n",
        "    print(\"Calling draft_email node...\")\n",
        "    # In a real agent, the LLM would generate draft_components based on state.\n",
        "    # For this structure definition, we'll simulate a draft update.\n",
        "    # A real agent would use a tool/LLM call here to get DraftedEmail components.\n",
        "    # For now, let's assume the agent somehow produces a draft.\n",
        "    # This part needs the LLM integration to decide what to draft.\n",
        "    # As a placeholder, let's update the body based on instructions.\n",
        "    instructions = state['environment_state'].user_instructions\n",
        "    current_draft = state['drafted_email'] or DraftedEmail()\n",
        "\n",
        "    # Simulate agent deciding to draft based on instructions\n",
        "    simulated_body_draft = f\"Draft based on instructions: {', '.join(instructions.key_points)}.\"\n",
        "    if instructions.tone:\n",
        "        simulated_body_draft = f\"Using a {instructions.tone} tone. \" + simulated_body_draft\n",
        "\n",
        "    updated_draft = DraftedEmail(\n",
        "        to_addresses=current_draft.to_addresses or ([instructions.recipient] if isinstance(instructions.recipient, str) else instructions.recipient) or [],\n",
        "        cc_addresses=current_draft.cc_addresses,\n",
        "        bcc_addresses=current_draft.bcc_addresses,\n",
        "        subject=current_draft.subject or instructions.subject or (f\"Re: {state['environment_state'].incoming_email.subject}\" if state['environment_state'].incoming_email else \"New Email\"),\n",
        "        body=simulated_body_draft # This is where LLM output would go\n",
        "    )\n",
        "\n",
        "    print(\"Drafting complete (simulated).\")\n",
        "    return {\"drafted_email\": updated_draft, \"messages\": [(\"tool_code\", f\"Drafted email subject: {updated_draft.subject[:50]}..., body: {updated_draft.body[:50]}...\")]}\n",
        "\n",
        "\n",
        "def call_send_email(state: AgentState) -> AgentState:\n",
        "    \"\"\"Calls the send_email_action and updates the state.\"\"\"\n",
        "    print(\"Calling send_email node...\")\n",
        "    drafted_email = state['drafted_email']\n",
        "    if not drafted_email or not drafted_email.body:\n",
        "         print(\"Cannot send empty email.\")\n",
        "         # Handle this case - perhaps transition to an error state or redraft\n",
        "         return {\"messages\": [(\"tool_code\", \"Attempted to send an empty email.\")]}\n",
        "\n",
        "    sent_email = send_email_action(drafted_email)\n",
        "    print(\"Email sent.\")\n",
        "    return {\"final_email_sent\": True, \"messages\": [(\"tool_code\", f\"Email sent: {sent_email.subject[:50]}...\")]}\n",
        "\n",
        "def call_search_information(state: AgentState) -> AgentState:\n",
        "    \"\"\"Calls the search_information_action and updates the state.\"\"\"\n",
        "    print(\"Calling search_information node...\")\n",
        "    # In a real agent, the LLM would decide the query based on state.\n",
        "    # For this structure definition, we'll simulate a query.\n",
        "    # A real agent would use a tool/LLM call here to get the query.\n",
        "    simulated_query = f\"information about {state['environment_state'].user_instructions.key_points[0] if state['environment_state'].user_instructions.key_points else 'general topic'}\"\n",
        "    print(f\"Simulated search query: {simulated_query}\")\n",
        "    search_results = search_information_action(simulated_query)\n",
        "    print(\"Search complete (simulated).\")\n",
        "    return {\"search_results\": search_results, \"messages\": [(\"tool_code\", f\"Search results: {search_results[:50]}...\")]}\n",
        "\n",
        "\n",
        "# Add nodes to the graph\n",
        "workflow.add_node(\"draft_email\", call_draft_email)\n",
        "workflow.add_node(\"send_email\", call_send_email)\n",
        "workflow.add_node(\"search_information\", call_search_information)\n",
        "\n",
        "# Define the entry point\n",
        "workflow.set_entry_point(\"draft_email\") # Start by drafting the email\n",
        "\n",
        "# Define edges and conditional transitions\n",
        "\n",
        "# After drafting, the agent might need to search or decide to send\n",
        "# This transition logic would be more complex in a real agent,\n",
        "# potentially involving an LLM call to decide the next step.\n",
        "# For this structure, let's add a simple condition: if search is needed\n",
        "# based on instructions (simulated), go to search, otherwise go to send.\n",
        "def should_search(state: AgentState) -> str:\n",
        "    \"\"\"Determines if the agent needs to search for information.\"\"\"\n",
        "    # Simulate the decision based on user instructions\n",
        "    instructions = state['environment_state'].user_instructions\n",
        "    # If user instructions mention needing info or a specific complex topic,\n",
        "    # the agent might decide to search.\n",
        "    # This is a placeholder condition.\n",
        "    if \"search_needed\" in instructions.other_criteria:\n",
        "        print(\"Decision: Search needed.\")\n",
        "        return \"search\"\n",
        "    else:\n",
        "        print(\"Decision: No search needed, proceed to send or redraft.\")\n",
        "        # In a real agent, this might go to a review/revise node or directly to send\n",
        "        # For this simplified graph, we'll go directly to send if no search.\n",
        "        return \"send\"\n",
        "\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"draft_email\",\n",
        "    should_search, # Use the function to decide the next step\n",
        "    {\"search\": \"search_information\", \"send\": \"send_email\"}\n",
        ")\n",
        "\n",
        "# After searching, the agent should go back to drafting to incorporate results\n",
        "workflow.add_edge(\"search_information\", \"draft_email\")\n",
        "\n",
        "# After sending the email, the process is finished\n",
        "workflow.add_edge(\"send_email\", END)\n",
        "\n",
        "\n",
        "# 4. Ensure the agent can access EmailEnvironmentState\n",
        "# The EmailEnvironmentState is part of the AgentState and is passed between nodes.\n",
        "\n",
        "# 5. Define the entry point and exit condition\n",
        "# Entry point is set to \"draft_email\"\n",
        "# Exit condition is reaching the END node after \"send_email\"\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph agent workflow defined.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langgraph'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-699562480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assuming SimulatedEmail, UserInstructions, EmailEnvironmentState,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langgraph'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a987a9f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `langgraph` library is not installed. Install the required package.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DERDLyJ9VYJ5",
        "outputId": "0fae16ee-3843-4f8a-c111-9e056a7a38b6"
      },
      "source": [
        "%pip install langgraph"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.74)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.3-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.3 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2c17413"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `langgraph` is installed, attempt to define the LangGraph agent architecture again using the previously defined actions and state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f93fc508",
        "outputId": "a66d44a0-17ff-4178-9a54-9778248ec24b"
      },
      "source": [
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict, Optional\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Assuming SimulatedEmail, UserInstructions, EmailEnvironmentState,\n",
        "# DraftedEmail, draft_email_action, send_email_action, and search_information_action\n",
        "# are defined in previous cells.\n",
        "\n",
        "# 1. Define the state graph\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of the LangGraph agent.\n",
        "\n",
        "    Attributes:\n",
        "        environment_state: The current state of the email environment.\n",
        "        drafted_email: The email being drafted by the agent.\n",
        "        messages: A list of messages representing the conversation history (for agent reasoning).\n",
        "        search_results: Results from information search (if performed).\n",
        "        final_email_sent: Flag indicating if the email has been sent.\n",
        "    \"\"\"\n",
        "    environment_state: EmailEnvironmentState\n",
        "    drafted_email: DraftedEmail\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    search_results: Optional[str]\n",
        "    final_email_sent: bool\n",
        "\n",
        "\n",
        "# 2. Implement the LangGraph workflow\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define nodes for the graph\n",
        "def call_draft_email(state: AgentState) -> AgentState:\n",
        "    \"\"\"Calls the draft_email_action and updates the state.\"\"\"\n",
        "    print(\"Calling draft_email node...\")\n",
        "    # In a real agent, the LLM would generate draft_components based on state.\n",
        "    # For this structure definition, we'll simulate a draft update.\n",
        "    # A real agent would use a tool/LLM call here to get DraftedEmail components.\n",
        "    # For now, let's assume the agent somehow produces a draft.\n",
        "    # This part needs the LLM integration to decide what to draft.\n",
        "    # As a placeholder, let's update the body based on instructions.\n",
        "    instructions = state['environment_state'].user_instructions\n",
        "    current_draft = state.get('drafted_email') or DraftedEmail(to_addresses=[], cc_addresses=[], bcc_addresses=[], subject=\"\", body=\"\") # Initialize if None\n",
        "\n",
        "    # Simulate agent deciding to draft based on instructions\n",
        "    simulated_body_draft = f\"Draft based on instructions: {', '.join(instructions.key_points)}.\"\n",
        "    if instructions.tone:\n",
        "        simulated_body_draft = f\"Using a {instructions.tone} tone. \" + simulated_body_draft\n",
        "\n",
        "    # Ensure recipient list is always a list\n",
        "    recipients = [instructions.recipient] if isinstance(instructions.recipient, str) else instructions.recipient if instructions.recipient is not None else []\n",
        "\n",
        "    updated_draft = DraftedEmail(\n",
        "        to_addresses=current_draft.to_addresses or recipients,\n",
        "        cc_addresses=current_draft.cc_addresses,\n",
        "        bcc_addresses=current_draft.bcc_addresses,\n",
        "        subject=current_draft.subject or instructions.subject or (f\"Re: {state['environment_state'].incoming_email.subject}\" if state['environment_state'].incoming_email else \"New Email\"),\n",
        "        body=simulated_body_draft # This is where LLM output would go\n",
        "    )\n",
        "\n",
        "    # Call the simulated action\n",
        "    drafted_email_result = draft_email_action(state['environment_state'], updated_draft)\n",
        "\n",
        "\n",
        "    print(\"Drafting complete (simulated).\")\n",
        "    return {\"drafted_email\": drafted_email_result, \"messages\": [(\"tool_code\", f\"Drafted email subject: {drafted_email_result.subject[:50]}..., body: {drafted_email_result.body[:50]}...\")]}\n",
        "\n",
        "\n",
        "def call_send_email(state: AgentState) -> AgentState:\n",
        "    \"\"\"Calls the send_email_action and updates the state.\"\"\"\n",
        "    print(\"Calling send_email node...\")\n",
        "    drafted_email = state.get('drafted_email')\n",
        "    if not drafted_email or not drafted_email.body:\n",
        "         print(\"Cannot send empty email.\")\n",
        "         # Handle this case - perhaps transition to an error state or redraft\n",
        "         return {\"messages\": [(\"tool_code\", \"Attempted to send an empty email.\")]}\n",
        "\n",
        "    sent_email = send_email_action(drafted_email)\n",
        "    print(\"Email sent.\")\n",
        "    return {\"final_email_sent\": True, \"messages\": [(\"tool_code\", f\"Email sent: {sent_email.subject[:50]}...\")]}\n",
        "\n",
        "def call_search_information(state: AgentState) -> AgentState:\n",
        "    \"\"\"Calls the search_information_action and updates the state.\"\"\"\n",
        "    print(\"Calling search_information node...\")\n",
        "    # In a real agent, the LLM would decide the query based on state.\n",
        "    # For this structure definition, we'll simulate a query.\n",
        "    # A real agent would use a tool/LLM call here to get the query.\n",
        "    simulated_query = f\"information about {state['environment_state'].user_instructions.key_points[0] if state['environment_state'].user_instructions.key_points else 'general topic'}\"\n",
        "    print(f\"Simulated search query: {simulated_query}\")\n",
        "    search_results = search_information_action(simulated_query)\n",
        "    print(\"Search complete (simulated).\")\n",
        "    return {\"search_results\": search_results, \"messages\": [(\"tool_code\", f\"Search results: {search_results[:50]}...\")]}\n",
        "\n",
        "\n",
        "# Add nodes to the graph\n",
        "workflow.add_node(\"draft_email\", call_draft_email)\n",
        "workflow.add_node(\"send_email\", call_send_email)\n",
        "workflow.add_node(\"search_information\", call_search_information)\n",
        "\n",
        "# Define the entry point\n",
        "workflow.set_entry_point(\"draft_email\") # Start by drafting the email\n",
        "\n",
        "# Define edges and conditional transitions\n",
        "\n",
        "# After drafting, the agent might need to search or decide to send\n",
        "# This transition logic would be more complex in a real agent,\n",
        "# potentially involving an LLM call to decide the next step.\n",
        "# For this structure, let's add a simple condition: if search is needed\n",
        "# based on instructions (simulated), go to search, otherwise go to send.\n",
        "def should_search(state: AgentState) -> str:\n",
        "    \"\"\"Determines if the agent needs to search for information.\"\"\"\n",
        "    # Simulate the decision based on user instructions\n",
        "    instructions = state['environment_state'].user_instructions\n",
        "    # If user instructions mention needing info or a specific complex topic,\n",
        "    # the agent might decide to search.\n",
        "    # This is a placeholder condition.\n",
        "    if instructions.other_criteria and \"search_needed\" in instructions.other_criteria and instructions.other_criteria[\"search_needed\"]:\n",
        "        print(\"Decision: Search needed.\")\n",
        "        return \"search\"\n",
        "    else:\n",
        "        print(\"Decision: No search needed, proceed to send or redraft.\")\n",
        "        # In a real agent, this might go to a review/revise node or directly to send\n",
        "        # For this simplified graph, we'll go directly to send if no search.\n",
        "        return \"send\"\n",
        "\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"draft_email\",\n",
        "    should_search, # Use the function to decide the next step\n",
        "    {\"search\": \"search_information\", \"send\": \"send_email\"}\n",
        ")\n",
        "\n",
        "# After searching, the agent should go back to drafting to incorporate results\n",
        "workflow.add_edge(\"search_information\", \"draft_email\")\n",
        "\n",
        "# After sending the email, the process is finished\n",
        "workflow.add_edge(\"send_email\", END)\n",
        "\n",
        "\n",
        "# 4. Ensure the agent can access EmailEnvironmentState\n",
        "# The EmailEnvironmentState is part of the AgentState and is passed between nodes.\n",
        "\n",
        "# 5. Define the entry point and exit condition\n",
        "# Entry point is set to \"draft_email\"\n",
        "# Exit condition is reaching the END node after \"send_email\"\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph agent workflow defined.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph agent workflow defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a586fb9e"
      },
      "source": [
        "## Implement the training loop\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop. Use a framework like ART (or a similar RL training setup) to train the agent. The agent would generate emails in the simulated environment, receive a reward based on the reward function, and update its policy to generate better emails over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20179b0a"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop using the ART framework to train the LangGraph agent defined in the previous step. This involves iterating through training scenarios, running the agent workflow, evaluating the generated email with the reward function, and updating the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "35b9f862",
        "outputId": "934dffc2-0565-4b01-cd0f-093f2a270d75"
      },
      "source": [
        "import art\n",
        "import asyncio\n",
        "from art.rewards import ruler_score_group # Assuming ruler_score_group can be adapted or a similar mechanism is used\n",
        "\n",
        "# Reuse the training scenarios loaded previously\n",
        "# training_scenarios = load_training_scenarios(split=\"train\", limit=50, max_messages=1, shuffle=True, seed=42)\n",
        "\n",
        "# Reuse the defined rollout function and model\n",
        "# from art.langgraph import wrap_rollout\n",
        "# model = art.TrainableModel(...) # Assumes model is already defined and registered\n",
        "# backend = LocalBackend(...) # Assumes backend is already defined\n",
        "# await model.register(backend) # Assumes model is registered\n",
        "# rollout = ... # Assumes rollout function is defined\n",
        "\n",
        "# Reuse the evaluate_email reward function\n",
        "# async def evaluate_email(...)\n",
        "\n",
        "# Reuse the LangGraph app (workflow)\n",
        "# app = workflow.compile()\n",
        "\n",
        "# 1. Set up the training configuration parameters\n",
        "# Using parameters similar to the example notebook, adapted for our task\n",
        "training_config = {\n",
        "    \"groups_per_step\": 2,       # Number of scenario groups processed per training step\n",
        "    \"num_epochs\": 5,            # Number of times to iterate through the dataset\n",
        "    \"rollouts_per_group\": 3,    # Number of times to run the agent for each scenario in a group\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"max_steps\": 5,             # Stop after this many training steps for the demo\n",
        "}\n",
        "\n",
        "print(\"Training configuration set.\")\n",
        "\n",
        "# 2. Define a mechanism to iterate through the training data scenarios.\n",
        "# Adapt the iterate_dataset function for our scenario structure\n",
        "class EmailScenarioBatchItem(art.BatchItem):\n",
        "    scenario: Dict[str, Any] # Store the scenario data including context and desired_output\n",
        "\n",
        "async def email_scenario_iterator(\n",
        "    scenarios: List[Dict[str, Any]],\n",
        "    groups_per_step: int,\n",
        "    num_epochs: int,\n",
        "    initial_step: int = 0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Iterates through email scenarios, yielding batches for training steps.\n",
        "\n",
        "    Args:\n",
        "        scenarios: A list of scenario dictionaries (context and desired_output).\n",
        "        groups_per_step: Number of scenario groups per step.\n",
        "        num_epochs: Number of epochs to iterate through the dataset.\n",
        "        initial_step: The starting training step number.\n",
        "\n",
        "    Yields:\n",
        "        Batch objects containing EmailScenarioBatchItems.\n",
        "    \"\"\"\n",
        "    scenario_count = len(scenarios)\n",
        "    scenarios_per_step = groups_per_step # Each scenario is a group of size 1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle scenarios at the start of each epoch\n",
        "        random.shuffle(scenarios)\n",
        "        print(f\"\\nStarting epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for i in range(0, scenario_count, scenarios_per_step):\n",
        "            current_scenarios = scenarios[i : i + scenarios_per_step]\n",
        "            current_step = initial_step + epoch * (scenario_count // scenarios_per_step) + (i // scenarios_per_step)\n",
        "\n",
        "            batch_items = [\n",
        "                EmailScenarioBatchItem(\n",
        "                    scenario=scenario_data,\n",
        "                    group_id=str(uuid.uuid4()), # Unique group ID for ART\n",
        "                    trajectory_id_prefix=str(uuid.uuid4()), # Prefix for trajectory IDs\n",
        "                ) for scenario_data in current_scenarios\n",
        "            ]\n",
        "\n",
        "            yield art.Batch(\n",
        "                step=current_step,\n",
        "                epoch=epoch + 1,\n",
        "                epoch_step=i // scenarios_per_step,\n",
        "                items=batch_items,\n",
        "            )\n",
        "\n",
        "\n",
        "print(\"Email scenario iterator defined.\")\n",
        "\n",
        "# Prepare the sample_training_dataset for the iterator\n",
        "# Ensure the scenario data structure matches what the iterator expects\n",
        "# (a list of dictionaries, each with 'context' and 'desired_output')\n",
        "# sample_training_dataset is already in this format from the previous step.\n",
        "training_data_for_iterator = sample_training_dataset # Use the previously created sample dataset\n",
        "\n",
        "# 3. Within the training loop, for each scenario in the current batch:\n",
        "# This is integrated into the gather_trajectory_groups and subsequent steps.\n",
        "\n",
        "# 4. Use the collected training examples to update the agent's policy.\n",
        "# 5. Optionally, include logging or monitoring.\n",
        "# 6. Continue the loop for the defined number of training steps.\n",
        "\n",
        "async def run_training_loop(model: art.TrainableModel, scenarios: List[Dict[str, Any]], training_config: Dict):\n",
        "    \"\"\"\n",
        "    Runs the main RL training loop using ART.\n",
        "\n",
        "    Args:\n",
        "        model: The ART trainable model.\n",
        "        scenarios: The list of training scenario dictionaries.\n",
        "        training_config: Dictionary of training configuration parameters.\n",
        "    \"\"\"\n",
        "    print(\"Starting training loop...\")\n",
        "\n",
        "    initial_step = await model.get_step()\n",
        "    print(f\"Initial model step: {initial_step}\")\n",
        "\n",
        "    # Use the custom email scenario iterator\n",
        "    training_iterator = email_scenario_iterator(\n",
        "        scenarios,\n",
        "        groups_per_step=training_config[\"groups_per_step\"],\n",
        "        num_epochs=training_config[\"num_epochs\"],\n",
        "        initial_step=initial_step,\n",
        "    )\n",
        "\n",
        "    for batch in training_iterator:\n",
        "        if batch.step > training_config[\"max_steps\"]:\n",
        "            print(f\"Reached max steps ({training_config['max_steps']}). Stopping training.\")\n",
        "            break\n",
        "\n",
        "        print(\n",
        "            f\"\\n--- Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step} ---\"\n",
        "        )\n",
        "        print(f\"Batch contains {len(batch.items)} scenarios.\")\n",
        "\n",
        "        # Create trajectory groups for this batch\n",
        "        groups = []\n",
        "        for batch_item in batch.items:\n",
        "            scenario_data = batch_item.scenario\n",
        "            environment_state_data = scenario_data[\"context\"]\n",
        "            desired_output_email_data = scenario_data[\"desired_output\"]\n",
        "\n",
        "            # Reconstruct Pydantic objects from dictionaries\n",
        "            environment_state = EmailEnvironmentState.model_validate(environment_state_data)\n",
        "            desired_output_email = SimulatedEmail.model_validate(desired_output_email_data) if desired_output_email_data else None\n",
        "\n",
        "            # Define the function to run for each rollout in the group\n",
        "            # This function needs to:\n",
        "            # 1. Initialize the LangGraph agent state with environment_state.\n",
        "            # 2. Run the LangGraph app (workflow).\n",
        "            # 3. Capture the final generated email from the agent state.\n",
        "            # 4. Evaluate the generated email using evaluate_email.\n",
        "            # 5. Create an ART Trajectory object with messages and the reward.\n",
        "\n",
        "            async def run_agent_rollout(model: art.Model, env_state: EmailEnvironmentState, desired_output: Optional[SimulatedEmail]) -> art.Trajectory:\n",
        "                \"\"\"Runs one agent rollout for a specific scenario.\"\"\"\n",
        "                print(\"Running agent rollout...\")\n",
        "                traj = art.Trajectory(\n",
        "                    reward=0.0, # Initial reward\n",
        "                    messages_and_choices=[], # Store interaction history\n",
        "                    metadata={\n",
        "                        \"scenario_id\": str(uuid.uuid4()), # Unique ID for this rollout\n",
        "                        \"batch_step\": batch.step,\n",
        "                        \"epoch\": batch.epoch,\n",
        "                        \"epoch_step\": batch.epoch_step,\n",
        "                        \"instructions_task_type\": env_state.user_instructions.task_type,\n",
        "                        # Add other relevant metadata from environment_state\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    # Initialize LangGraph state\n",
        "                    initial_langgraph_state = AgentState(\n",
        "                        environment_state=env_state,\n",
        "                        drafted_email=DraftedEmail(to_addresses=[], cc_addresses=[], bcc_addresses=[], subject=\"\", body=\"\"), # Start with empty draft\n",
        "                        messages=[], # Start with no messages\n",
        "                        search_results=None,\n",
        "                        final_email_sent=False,\n",
        "                    )\n",
        "\n",
        "                    # Run the LangGraph app\n",
        "                    # Note: Running LangGraph compiled app directly here.\n",
        "                    # The nodes within the app will use the actions (draft, send, search).\n",
        "                    # We need to capture the final state/output from running the app.\n",
        "                    # LangGraph's `invoke` or `ainvoke` returns the final state.\n",
        "                    final_langgraph_state = await app.ainvoke(initial_langgraph_state)\n",
        "\n",
        "                    # Capture the final generated email from the state\n",
        "                    final_generated_email = final_langgraph_state.get('drafted_email') # Assuming the last state before END has the final draft\n",
        "\n",
        "                    # Check if email was actually sent based on workflow\n",
        "                    email_was_sent = final_langgraph_state.get('final_email_sent', False)\n",
        "\n",
        "                    if final_generated_email and email_was_sent:\n",
        "                        print(\"Agent successfully generated and sent an email.\")\n",
        "                        # Evaluate the generated email to get the reward\n",
        "                        reward = await evaluate_email(\n",
        "                            final_generated_email,\n",
        "                            env_state,\n",
        "                            desired_output # Pass the desired output for comparison\n",
        "                        )\n",
        "                        traj.reward = reward\n",
        "                        traj.metrics[\"email_sent\"] = 1.0\n",
        "                        traj.metadata[\"final_subject\"] = final_generated_email.subject\n",
        "                        traj.metadata[\"final_body_snippet\"] = final_generated_email.body[:100] + \"...\" if len(final_generated_email.body) > 100 else final_generated_email.body\n",
        "\n",
        "                        # You might want to capture the messages/tool calls from the LangGraph run\n",
        "                        # This requires integrating LangGraph's tracing or manual logging within nodes\n",
        "                        # For simplicity here, we'll just note that the email was sent.\n",
        "                        traj.messages_and_choices.append({\"role\": \"user\", \"content\": \"Simulated Email Task Started\"})\n",
        "                        traj.messages_and_choices.append({\"role\": \"assistant\", \"content\": f\"Simulated Agent Run Completed. Email Subject: {final_generated_email.subject}\"})\n",
        "\n",
        "\n",
        "                    else:\n",
        "                        print(\"Agent did not successfully send an email.\")\n",
        "                        traj.reward = 0.0 # Zero reward if email wasn't sent or drafted properly\n",
        "                        traj.metrics[\"email_sent\"] = 0.0\n",
        "                        traj.messages_and_choices.append({\"role\": \"user\", \"content\": \"Simulated Email Task Started\"})\n",
        "                        traj.messages_and_choices.append({\"role\": \"assistant\", \"content\": \"Simulated Agent Run Failed to Send Email.\"})\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during agent rollout: {e}\")\n",
        "                    traj.reward = 0.0 # Penalize errors\n",
        "                    traj.metrics[\"email_sent\"] = 0.0\n",
        "                    traj.messages_and_choices.append({\"role\": \"user\", \"content\": \"Simulated Email Task Started\"})\n",
        "                    traj.messages_and_choices.append({\"role\": \"assistant\", \"content\": f\"Error during rollout: {str(e)}\"})\n",
        "\n",
        "\n",
        "                return traj\n",
        "\n",
        "            # Create a group with multiple rollouts for this scenario\n",
        "            groups.append(\n",
        "                art.TrajectoryGroup(\n",
        "                    (\n",
        "                        run_agent_rollout(model, environment_state, desired_output)\n",
        "                        for _ in range(training_config[\"rollouts_per_group\"])\n",
        "                    ),\n",
        "                    group_id=batch_item.group_id,\n",
        "                    trajectory_id_prefix=batch_item.trajectory_id_prefix,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        print(f\"Gathering {len(groups)} trajectory groups with {training_config['rollouts_per_group']} rollouts each...\")\n",
        "        # Gather all trajectory groups in parallel\n",
        "        # Using asyncio.gather to run all group futures\n",
        "        finished_groups = await asyncio.gather(*(group.as_future() for group in groups))\n",
        "        print(\"Finished gathering trajectory groups.\")\n",
        "\n",
        "        # RULER Scoring (or use the direct reward from evaluate_email)\n",
        "        # If evaluate_email already provides a score from 0-1, you might directly use that\n",
        "        # instead of relative ranking with ruler_score_group.\n",
        "        # However, RULER can still be used for comparing multiple trajectories for the *same* scenario\n",
        "        # which is what rollouts_per_group provides.\n",
        "\n",
        "        judged_groups = []\n",
        "        print(\"Scoring trajectory groups with RULER...\")\n",
        "        for group in finished_groups:\n",
        "            # Use ruler_score_group if you want relative ranking within the group.\n",
        "            # This requires the LLM judge to compare trajectories directly,\n",
        "            # which is different from our evaluate_email that scores one email at a time\n",
        "            # against criteria/desired output.\n",
        "            # Option A: If evaluate_email gives a good absolute score (0-1),\n",
        "            # use that directly and skip ruler_score_group.\n",
        "            # Option B: Adapt ruler_score_group to compare the *final emails*\n",
        "            # generated in each trajectory for the same scenario. This is more complex.\n",
        "\n",
        "            # Let's assume for simplicity that evaluate_email provides a sufficient\n",
        "            # score, and we'll use that directly. ART's train function can handle\n",
        "            # trajectories with pre-assigned rewards.\n",
        "            # If using RULER for relative ranking, you'd call it here:\n",
        "            # judged_group = await ruler_score_group(group, \"openai/o4-mini\", debug=False)\n",
        "            # judged_groups.append(judged_group)\n",
        "\n",
        "            # Using Option A: Directly use the reward from evaluate_email\n",
        "            judged_groups.append(group) # Pass the group as is, assuming rewards are set\n",
        "\n",
        "        print(\"Finished scoring trajectory groups.\")\n",
        "\n",
        "        # Log metrics from the trajectories (optional)\n",
        "        all_trajectories_in_batch = [t for group in judged_groups for t in group.trajectories]\n",
        "        avg_reward = sum(t.reward for t in all_trajectories_in_batch) / max(len(all_trajectories_in_batch), 1)\n",
        "        avg_email_sent = sum(t.metrics.get(\"email_sent\", 0.0) for t in all_trajectories_in_batch) / max(len(all_trajectories_in_batch), 1)\n",
        "        print(f\"Batch average reward: {avg_reward:.4f}\")\n",
        "        print(f\"Batch email sent rate: {avg_email_sent:.2f}\")\n",
        "\n",
        "        # 4. Use the collected training examples to update the agent's policy.\n",
        "        # The ART train function takes the scored trajectory groups.\n",
        "        print(\"Starting model training step...\")\n",
        "        await model.train(\n",
        "            judged_groups,\n",
        "            config=art.TrainConfig(learning_rate=training_config[\"learning_rate\"]),\n",
        "             # Add any necessary internal configs for model training\n",
        "             _config={\"logprob_calculation_chunk_size\": 8} # Example from previous notebook\n",
        "        )\n",
        "        print(f\"Completed training step {batch.step}.\")\n",
        "\n",
        "    print(\"Training loop finished.\")\n",
        "\n",
        "# To run the training loop, you would await this function call:\n",
        "# await run_training_loop(model, training_data_for_iterator, training_config)\n",
        "\n",
        "print(\"Training loop function defined.\")\n",
        "\n",
        "# Note: Running the full training loop here would take significant time and resources.\n",
        "# The code block defines the function `run_training_loop` which contains the logic.\n",
        "# To execute it, you would need to call `await run_training_loop(...)` in a subsequent cell\n",
        "# after ensuring the `model`, `training_data_for_iterator`, and `training_config` are available.\n",
        "# For the purpose of this subtask (implementing the loop), defining the function is sufficient.\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'art'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3582919466.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mruler_score_group\u001b[0m \u001b[0;31m# Assuming ruler_score_group can be adapted or a similar mechanism is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Reuse the training scenarios loaded previously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'art'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b99d4e1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `art` library was not found. Although ART was installed in a prior cell, the kernel state seems to have reset or the import was not successful in the new execution context. Re-importing the necessary libraries, including `art`, and then executing the training loop logic is required to proceed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "53tiZW19VsBh",
        "outputId": "358bb3c5-5331-4c72-f34d-956b28f2f9c2"
      },
      "source": [
        "import art\n",
        "import asyncio\n",
        "import random\n",
        "import uuid\n",
        "from typing import Dict, Any, List, Optional\n",
        "\n",
        "# Assuming SimulatedEmail, UserInstructions, EmailEnvironmentState,\n",
        "# DraftedEmail, draft_email_action, send_email_action, and search_information_action\n",
        "# are defined in previous cells.\n",
        "\n",
        "# Assuming sample_training_dataset is defined in a previous cell\n",
        "\n",
        "# Assuming evaluate_email function is defined in a previous cell\n",
        "\n",
        "# Assuming the LangGraph app (workflow) is compiled and available as `app`\n",
        "\n",
        "# Assuming model and backend are defined and the model is registered\n",
        "\n",
        "# Reuse the training scenarios loaded previously\n",
        "# training_scenarios = load_training_scenarios(split=\"train\", limit=50, max_messages=1, shuffle=True, seed=42)\n",
        "# NOTE: Using sample_training_dataset which has the structure {context: {...}, desired_output: {...}}\n",
        "\n",
        "# 1. Set up the training configuration parameters\n",
        "# Using parameters similar to the example notebook, adapted for our task\n",
        "training_config = {\n",
        "    \"groups_per_step\": 2,       # Number of scenario groups processed per training step\n",
        "    \"num_epochs\": 5,            # Number of times to iterate through the dataset\n",
        "    \"rollouts_per_group\": 3,    # Number of times to run the agent for each scenario in a group\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"max_steps\": 5,             # Stop after this many training steps for the demo\n",
        "}\n",
        "\n",
        "print(\"Training configuration set.\")\n",
        "\n",
        "# 2. Define a mechanism to iterate through the training data scenarios.\n",
        "class EmailScenarioBatchItem(art.BatchItem):\n",
        "    scenario: Dict[str, Any] # Store the scenario data including context and desired_output\n",
        "\n",
        "async def email_scenario_iterator(\n",
        "    scenarios: List[Dict[str, Any]],\n",
        "    groups_per_step: int,\n",
        "    num_epochs: int,\n",
        "    initial_step: int = 0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Iterates through email scenarios, yielding batches for training steps.\n",
        "\n",
        "    Args:\n",
        "        scenarios: A list of scenario dictionaries (context and desired_output).\n",
        "        groups_per_step: Number of scenario groups per step.\n",
        "        num_epochs: Number of epochs to iterate through the dataset.\n",
        "        initial_step: The starting training step number.\n",
        "\n",
        "    Yields:\n",
        "        Batch objects containing EmailScenarioBatchItems.\n",
        "    \"\"\"\n",
        "    scenario_count = len(scenarios)\n",
        "    scenarios_per_step = groups_per_step # Each scenario is a group of size 1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle scenarios at the start of each epoch\n",
        "        random.shuffle(scenarios)\n",
        "        print(f\"\\nStarting epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for i in range(0, scenario_count, scenarios_per_step):\n",
        "            current_scenarios = scenarios[i : i + scenarios_per_step]\n",
        "            current_step = initial_step + epoch * (scenario_count // scenarios_per_step) + (i // scenarios_per_step)\n",
        "\n",
        "            batch_items = [\n",
        "                EmailScenarioBatchItem(\n",
        "                    scenario=scenario_data,\n",
        "                    group_id=str(uuid.uuid4()), # Unique group ID for ART\n",
        "                    trajectory_id_prefix=str(uuid.uuid4()), # Prefix for trajectory IDs\n",
        "                ) for scenario_data in current_scenarios\n",
        "            ]\n",
        "\n",
        "            yield art.Batch(\n",
        "                step=current_step,\n",
        "                epoch=epoch + 1,\n",
        "                epoch_step=i // scenarios_per_step,\n",
        "                items=batch_items,\n",
        "            )\n",
        "\n",
        "\n",
        "print(\"Email scenario iterator defined.\")\n",
        "\n",
        "# Prepare the sample_training_dataset for the iterator\n",
        "# Ensure the scenario data structure matches what the iterator expects\n",
        "# (a list of dictionaries, each with 'context' and 'desired_output')\n",
        "# sample_training_dataset is already in this format from the previous step.\n",
        "training_data_for_iterator = sample_training_dataset # Use the previously created sample dataset\n",
        "\n",
        "# 3. Within the training loop, for each scenario in the current batch:\n",
        "# This is integrated into the gather_trajectory_groups and subsequent steps.\n",
        "\n",
        "# 4. Use the collected training examples to update the agent's policy.\n",
        "# 5. Optionally, include logging or monitoring.\n",
        "# 6. Continue the loop for the defined number of training steps.\n",
        "\n",
        "async def run_agent_rollout_with_langgraph(model: art.Model, env_state: EmailEnvironmentState, desired_output: Optional[SimulatedEmail], langgraph_app, batch_info: Dict) -> art.Trajectory:\n",
        "    \"\"\"Runs one agent rollout for a specific scenario using the LangGraph app.\"\"\"\n",
        "    print(\"Running agent rollout with LangGraph...\")\n",
        "    traj = art.Trajectory(\n",
        "        reward=0.0, # Initial reward\n",
        "        messages_and_choices=[], # Store interaction history\n",
        "        metadata={\n",
        "            \"scenario_id\": str(uuid.uuid4()), # Unique ID for this rollout\n",
        "            \"batch_step\": batch_info.get(\"step\"),\n",
        "            \"epoch\": batch_info.get(\"epoch\"),\n",
        "            \"epoch_step\": batch_info.get(\"epoch_step\"),\n",
        "            \"instructions_task_type\": env_state.user_instructions.task_type,\n",
        "            # Add other relevant metadata from environment_state\n",
        "        },\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Initialize LangGraph state\n",
        "        initial_langgraph_state = AgentState(\n",
        "            environment_state=env_state,\n",
        "            drafted_email=DraftedEmail(to_addresses=[], cc_addresses=[], bcc_addresses=[], subject=\"\", body=\"\"), # Start with empty draft\n",
        "            messages=[], # Start with no messages\n",
        "            search_results=None,\n",
        "            final_email_sent=False,\n",
        "        )\n",
        "\n",
        "        # Run the LangGraph app\n",
        "        # LangGraph's `invoke` or `ainvoke` returns the final state.\n",
        "        # We need to capture the messages/tool calls from the LangGraph run\n",
        "        # This might require integrating LangGraph's tracing or manual logging within nodes\n",
        "        # For simplicity here, we'll just note the start and end of the run and capture final state.\n",
        "        traj.messages_and_choices.append({\"role\": \"system\", \"content\": \"LangGraph Agent Run Started\"})\n",
        "\n",
        "        # Note: The LangGraph app `app` needs to be available in this scope.\n",
        "        # The nodes within `app` use the actions (draft_email_action, send_email_action, search_information_action).\n",
        "        # These action functions need to access the environment_state from the LangGraph state.\n",
        "        # The current action functions are defined globally and take env_state as an argument.\n",
        "        # The LangGraph nodes defined previously (`call_draft_email`, etc.) already handle passing the env_state.\n",
        "\n",
        "        final_langgraph_state = await langgraph_app.ainvoke(initial_langgraph_state)\n",
        "\n",
        "        # Capture interaction history from the messages list in the final state\n",
        "        # Assuming the LangGraph state's 'messages' key contains the history\n",
        "        if 'messages' in final_langgraph_state and isinstance(final_langgraph_state['messages'], list):\n",
        "             traj.messages_and_choices.extend([{\"role\": msg[0] if isinstance(msg, tuple) else \"unknown\", \"content\": str(msg[1] if isinstance(msg, tuple) else msg)} for msg in final_langgraph_state['messages']])\n",
        "\n",
        "\n",
        "        # Capture the final generated email from the state\n",
        "        final_generated_email_dict = final_langgraph_state.get('drafted_email')\n",
        "        final_generated_email = DraftedEmail.model_validate(final_generated_email_dict) if final_generated_email_dict else None\n",
        "\n",
        "        # Check if email was actually sent based on workflow\n",
        "        email_was_sent = final_langgraph_state.get('final_email_sent', False)\n",
        "\n",
        "        if final_generated_email and email_was_sent:\n",
        "            print(\"Agent successfully generated and sent an email.\")\n",
        "            # Evaluate the generated email to get the reward\n",
        "            reward = await evaluate_email(\n",
        "                SimulatedEmail( # Convert DraftedEmail to SimulatedEmail for evaluation\n",
        "                    message_id=\"generated\", # Placeholder ID\n",
        "                    date=env_state.current_date, # Use simulation date\n",
        "                    subject=final_generated_email.subject,\n",
        "                    from_address=\"simulated_user@example.com\", # Assume user's address\n",
        "                    to_addresses=final_generated_email.to_addresses,\n",
        "                    cc_addresses=final_generated_email.cc_addresses,\n",
        "                    bcc_addresses=final_generated_email.bcc_addresses,\n",
        "                    body=final_generated_email.body,\n",
        "                    attachments=[]\n",
        "                ),\n",
        "                env_state,\n",
        "                desired_output # Pass the desired output for comparison\n",
        "            )\n",
        "            traj.reward = reward\n",
        "            traj.metrics[\"email_sent\"] = 1.0\n",
        "            traj.metadata[\"final_subject\"] = final_generated_email.subject\n",
        "            traj.metadata[\"final_body_snippet\"] = final_generated_email.body[:100] + \"...\" if len(final_generated_email.body) > 100 else final_generated_email.body\n",
        "\n",
        "        else:\n",
        "            print(\"Agent did not successfully send an email.\")\n",
        "            traj.reward = 0.0 # Zero reward if email wasn't sent or drafted properly\n",
        "            traj.metrics[\"email_sent\"] = 0.0\n",
        "            traj.messages_and_choices.append({\"role\": \"assistant\", \"content\": \"Simulated Agent Run Failed to Send Email.\"})\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during agent rollout: {e}\")\n",
        "        traj.reward = 0.0 # Penalize errors\n",
        "        traj.metrics[\"email_sent\"] = 0.0\n",
        "        traj.messages_and_choices.append({\"role\": \"assistant\", \"content\": f\"Error during rollout: {str(e)}\"})\n",
        "\n",
        "\n",
        "    return traj\n",
        "\n",
        "\n",
        "async def run_training_loop(model: art.TrainableModel, scenarios: List[Dict[str, Any]], training_config: Dict, langgraph_app):\n",
        "    \"\"\"\n",
        "    Runs the main RL training loop using ART and a LangGraph agent.\n",
        "\n",
        "    Args:\n",
        "        model: The ART trainable model.\n",
        "        scenarios: The list of training scenario dictionaries.\n",
        "        training_config: Dictionary of training configuration parameters.\n",
        "        langgraph_app: The compiled LangGraph workflow.\n",
        "    \"\"\"\n",
        "    print(\"Starting training loop...\")\n",
        "\n",
        "    initial_step = await model.get_step()\n",
        "    print(f\"Initial model step: {initial_step}\")\n",
        "\n",
        "    # Use the custom email scenario iterator\n",
        "    training_iterator = email_scenario_iterator(\n",
        "        scenarios,\n",
        "        groups_per_step=training_config[\"groups_per_step\"],\n",
        "        num_epochs=training_config[\"num_epochs\"],\n",
        "        initial_step=initial_step,\n",
        "    )\n",
        "\n",
        "    async for batch in training_iterator:\n",
        "        if batch.step > training_config[\"max_steps\"]:\n",
        "            print(f\"Reached max steps ({training_config['max_steps']}). Stopping training.\")\n",
        "            break\n",
        "\n",
        "        print(\n",
        "            f\"\\n--- Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step} ---\"\n",
        "        )\n",
        "        print(f\"Batch contains {len(batch.items)} scenarios.\")\n",
        "\n",
        "        # Create trajectory groups for this batch\n",
        "        groups = []\n",
        "        for batch_item in batch.items:\n",
        "            scenario_data = batch_item.scenario\n",
        "            environment_state_data = scenario_data[\"context\"]\n",
        "            desired_output_email_data = scenario_data[\"desired_output\"]\n",
        "\n",
        "            # Reconstruct Pydantic objects from dictionaries\n",
        "            environment_state = EmailEnvironmentState.model_validate(environment_state_data)\n",
        "            desired_output_email = SimulatedEmail.model_validate(desired_output_email_data) if desired_output_email_data else None\n",
        "\n",
        "            # Create a group with multiple rollouts for this scenario\n",
        "            # Each rollout runs the LangGraph agent\n",
        "            groups.append(\n",
        "                art.TrajectoryGroup(\n",
        "                    (\n",
        "                        run_agent_rollout_with_langgraph(\n",
        "                            model, # Pass the ART model if needed within the rollout (e.g., for inference)\n",
        "                            environment_state,\n",
        "                            desired_output,\n",
        "                            langgraph_app, # Pass the compiled LangGraph app\n",
        "                            {\"step\": batch.step, \"epoch\": batch.epoch, \"epoch_step\": batch.epoch_step} # Pass batch info for metadata\n",
        "                        ) for _ in range(training_config[\"rollouts_per_group\"])\n",
        "                    ),\n",
        "                    group_id=batch_item.group_id,\n",
        "                    trajectory_id_prefix=batch_item.trajectory_id_prefix,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        print(f\"Gathering {len(groups)} trajectory groups with {training_config['rollouts_per_group']} rollouts each...\")\n",
        "        # Gather all trajectory groups in parallel\n",
        "        # Using asyncio.gather to run all group futures\n",
        "        finished_groups = await asyncio.gather(*(group.as_future() for group in groups))\n",
        "        print(\"Finished gathering trajectory groups.\")\n",
        "\n",
        "        # Score trajectories\n",
        "        judged_groups = []\n",
        "        print(\"Processing gathered trajectory groups...\")\n",
        "        for group in finished_groups:\n",
        "            # Since evaluate_email provides a score per rollout, we can use that directly.\n",
        "            # If relative ranking within the group is desired, ruler_score_group would be called here.\n",
        "            # For this task, we'll assume evaluate_email's score is sufficient.\n",
        "            judged_groups.append(group) # Pass the group as is, assuming rewards are set by run_agent_rollout_with_langgraph\n",
        "\n",
        "        print(\"Finished processing trajectory groups.\")\n",
        "\n",
        "        # Log metrics from the trajectories (optional)\n",
        "        all_trajectories_in_batch = [t for group in judged_groups for t in group.trajectories]\n",
        "        avg_reward = sum(t.reward for t in all_trajectories_in_batch) / max(len(all_trajectories_in_batch), 1)\n",
        "        avg_email_sent = sum(t.metrics.get(\"email_sent\", 0.0) for t in all_trajectories_in_batch) / max(len(all_trajectories_in_batch), 1)\n",
        "        print(f\"Batch average reward: {avg_reward:.4f}\")\n",
        "        print(f\"Batch email sent rate: {avg_email_sent:.2f}\")\n",
        "\n",
        "        # 4. Use the collected training examples to update the agent's policy.\n",
        "        # The ART train function takes the scored trajectory groups.\n",
        "        print(\"Starting model training step...\")\n",
        "        await model.train(\n",
        "            judged_groups,\n",
        "            config=art.TrainConfig(learning_rate=training_config[\"learning_rate\"]),\n",
        "             # Add any necessary internal configs for model training\n",
        "             _config={\"logprob_calculation_chunk_size\": 8} # Example from previous notebook\n",
        "        )\n",
        "        print(f\"Completed training step {batch.step}.\")\n",
        "\n",
        "        # Optionally, save model checkpoints periodically\n",
        "        # await model.save_checkpoint() # Uncomment to save checkpoints\n",
        "\n",
        "    print(\"Training loop finished.\")\n",
        "\n",
        "# To run the training loop, you would await this function call:\n",
        "# await run_training_loop(model, training_data_for_iterator, training_config, app)\n",
        "\n",
        "print(\"Full training loop function `run_training_loop` defined.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'art'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-255587510.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'art'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960c9572"
      },
      "source": [
        "**Reasoning**:\n",
        "The `art` module is still not found, despite attempting to re-import it. This suggests an environment issue where the installed packages are not accessible in the current kernel session. A potential solution is to explicitly run the installation command again in a separate cell to ensure the package is available before attempting the import and defining the training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0dh4oamVt6C",
        "outputId": "1cf929a0-38de-44cc-8f42-cb01b3ddb004"
      },
      "source": [
        "%pip install \"openpipe-art[backend,langgraph]==0.4.9\" langchain-core langgraph langchain_openai tenacity datasets \"gql<4\" --prerelease allow --no-cache-dir"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --prerelease\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e0eb71e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `--prerelease` option is not recognized by the `pip install` command. This option was likely intended for `uv pip install` as seen in the initial setup. The standard `pip install` command should be used without this option to install the packages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b88f71e0",
        "outputId": "449f2ffe-b457-4287-e77f-547290d69049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%pip install \"openpipe-art[backend,langgraph]==0.4.9\" langchain-core langgraph langchain_openai tenacity datasets \"gql<4\" --no-cache-dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openpipe-art==0.4.9 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading openpipe_art-0.4.9-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.74)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (0.6.6)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.12/dist-packages (8.5.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting gql<4\n",
            "  Downloading gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting litellm==1.74.1 (from openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading litellm-1.74.1-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<=1.99.1,>=1.65.5 (from openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading openai-1.99.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: typer>=0.15.2 in /usr/local/lib/python3.12/dist-packages (from openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.16.0)\n",
            "Collecting weave>=0.51.51 (from openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading weave-0.52.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting accelerate==1.7.0 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting awscli>=1.38.1 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading awscli-1.42.17-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes>=0.45.2 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: hf-xet>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from openpipe-art[backend,langgraph]==0.4.9) (1.1.7)\n",
            "Requirement already satisfied: nbclient>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from openpipe-art[backend,langgraph]==0.4.9) (0.10.2)\n",
            "Collecting nbmake>=1.5.5 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading nbmake-1.5.5-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: peft>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from openpipe-art[backend,langgraph]==0.4.9) (0.17.0)\n",
            "Collecting polars>=1.26.0 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading polars-1.32.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pytest>=8.4.1 in /usr/local/lib/python3.12/dist-packages (from openpipe-art[backend,langgraph]==0.4.9) (8.4.1)\n",
            "Collecting setproctitle>=1.3.6 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading setproctitle-1.3.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting setuptools>=78.1.0 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tblib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from openpipe-art[backend,langgraph]==0.4.9) (3.1.0)\n",
            "Requirement already satisfied: torch>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from openpipe-art[backend,langgraph]==0.4.9) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchao>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from openpipe-art[backend,langgraph]==0.4.9) (0.10.0)\n",
            "Requirement already satisfied: torchtune in /usr/local/lib/python3.12/dist-packages (from openpipe-art[backend,langgraph]==0.4.9) (0.6.1)\n",
            "Collecting transformers==4.53.2 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m204.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl==0.20.0 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading trl-0.20.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting unsloth-zoo==2025.8.5 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading unsloth_zoo-2025.8.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting unsloth==2025.8.6 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading unsloth-2025.8.6-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m192.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting vllm<=0.10.0,>=0.9.2 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading vllm-0.10.0-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "Collecting wandb==0.21.0 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.7.0->openpipe-art[backend,langgraph]==0.4.9) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.7.0->openpipe-art[backend,langgraph]==0.4.9) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.7.0->openpipe-art[backend,langgraph]==0.4.9) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==1.7.0->openpipe-art[backend,langgraph]==0.4.9) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.7.0->openpipe-art[backend,langgraph]==0.4.9) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.7.0->openpipe-art[backend,langgraph]==0.4.9) (0.6.2)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (3.12.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (4.25.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (2.11.7)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (1.1.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.11.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.21.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.53.2->openpipe-art[backend,langgraph]==0.4.9) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.53.2->openpipe-art[backend,langgraph]==0.4.9) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.53.2->openpipe-art[backend,langgraph]==0.4.9) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.53.2->openpipe-art[backend,langgraph]==0.4.9) (4.67.1)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (3.4.0)\n",
            "Collecting tyro (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading tyro-0.9.28-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (0.2.1)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (0.45.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (5.29.5)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (0.34.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (0.23.0+cu126)\n",
            "Collecting cut_cross_entropy (from unsloth-zoo==2025.8.5->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth-zoo==2025.8.5->openpipe-art[backend,langgraph]==0.4.9) (11.3.0)\n",
            "Collecting msgspec (from unsloth-zoo==2025.8.5->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from unsloth-zoo==2025.8.5->openpipe-art[backend,langgraph]==0.4.9) (4.14.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.21.0->openpipe-art[backend,langgraph]==0.4.9) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb==0.21.0->openpipe-art[backend,langgraph]==0.4.9) (4.3.8)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb==0.21.0->openpipe-art[backend,langgraph]==0.4.9) (2.35.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading langchain_openai-0.3.29-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Collecting graphql-core<3.2.7,>=3.2 (from gql<4)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.12/dist-packages (from gql<4) (1.20.1)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql<4)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gql<4) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.0->gql<4) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.0->gql<4) (1.3.1)\n",
            "Collecting botocore==1.40.17 (from awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading botocore-1.40.17-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting docutils<=0.19,>=0.18.1 (from awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting colorama<0.4.7,>=0.2.5 (from awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.40.17->awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore==1.40.17->awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore==1.40.17->awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9) (2.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (0.24.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from nbclient>=0.10.1->openpipe-art[backend,langgraph]==0.4.9) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbclient>=0.10.1->openpipe-art[backend,langgraph]==0.4.9) (5.8.1)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbclient>=0.10.1->openpipe-art[backend,langgraph]==0.4.9) (5.10.4)\n",
            "Requirement already satisfied: traitlets>=5.4 in /usr/local/lib/python3.12/dist-packages (from nbclient>=0.10.1->openpipe-art[backend,langgraph]==0.4.9) (5.7.1)\n",
            "Requirement already satisfied: ipykernel>=5.4.0 in /usr/local/lib/python3.12/dist-packages (from nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (6.17.1)\n",
            "Requirement already satisfied: pygments>=2.7.3 in /usr/local/lib/python3.12/dist-packages (from nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (2.19.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<=1.99.1,>=1.65.5->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<=1.99.1,>=1.65.5->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.4.1)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.4.1->openpipe-art[backend,langgraph]==0.4.9) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.4.1->openpipe-art[backend,langgraph]==0.4.9) (1.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.53.2->openpipe-art[backend,langgraph]==0.4.9) (3.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.53.2->openpipe-art[backend,langgraph]==0.4.9) (2025.8.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (1.11.1.6)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.2->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.2->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (13.9.4)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (5.5.2)\n",
            "Collecting blake3 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (9.0.0)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.116.1)\n",
            "Collecting openai<=1.99.1,>=1.65.5 (from openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading openai-1.90.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.22.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading lm_format_enforcer-0.10.12-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines_core==0.2.10 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading outlines_core-0.2.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting diskcache==5.6.3 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lark==1.2.2 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.21 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading xgrammar-0.1.21-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting partial-json-parser (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (26.2.1)\n",
            "Collecting gguf>=0.13.0 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading mistral_common-1.8.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (1.17.0)\n",
            "Collecting setuptools>=78.1.0 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.8.1)\n",
            "Collecting compressed-tensors==0.10.2 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading compressed_tensors-0.10.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.19.0 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (3.1.1)\n",
            "Collecting watchfiles (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting python-json-logger (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (1.16.1)\n",
            "Collecting ninja (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pybase64 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting cbor2 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading cbor2-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting numba==0.61.2 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading ray-2.48.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting torch>=2.7.0 (from openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchaudio==2.7.1 (from vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading torchaudio-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchvision (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting astor (from depyf==0.19.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting eval-type-backport (from weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.12/dist-packages (from weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (1.6.0)\n",
            "Collecting polyfile-weave (from weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading polyfile_weave-0.5.6-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql<4) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql<4) (0.3.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: torchdata==0.11.0 in /usr/local/lib/python3.12/dist-packages (from torchtune->openpipe-art[backend,langgraph]==0.4.9) (0.11.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from torchtune->openpipe-art[backend,langgraph]==0.4.9) (0.3.12)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.12/dist-packages (from torchtune->openpipe-art[backend,langgraph]==0.4.9) (3.0.0)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from torchtune->openpipe-art[backend,langgraph]==0.4.9) (2.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.12/dist-packages (from blobfile>=2->torchtune->openpipe-art[backend,langgraph]==0.4.9) (3.23.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.12/dist-packages (from blobfile>=2->torchtune->openpipe-art[backend,langgraph]==0.4.9) (5.4.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.47.2)\n",
            "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading fastapi_cli-0.0.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.35.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.21.0->openpipe-art[backend,langgraph]==0.4.9) (4.0.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (3.23.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (7.34.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (0.1.7)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (6.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.27.0)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer<0.11,>=0.10.11->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.1->nbclient>=0.10.1->openpipe-art[backend,langgraph]==0.4.9) (2.21.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (1.1.1)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (13.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.15.2->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (4.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<4.8,>=3.1.2->awscli>=1.38.1->openpipe-art[backend,langgraph]==0.4.9) (0.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.7.0->openpipe-art[backend,langgraph]==0.4.9) (1.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->torchtune->openpipe-art[backend,langgraph]==0.4.9) (4.9.3)\n",
            "Collecting abnf~=2.2.0 (from polyfile-weave->weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading abnf-2.2.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: chardet>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (5.2.0)\n",
            "Collecting cint>=1.0.0 (from polyfile-weave->weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading cint-1.0.0-py3-none-any.whl.metadata (511 bytes)\n",
            "Collecting fickling>=0.0.8 (from polyfile-weave->weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading fickling-0.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: graphviz>=0.20.1 in /usr/local/lib/python3.12/dist-packages (from polyfile-weave->weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.21)\n",
            "Collecting intervaltree>=2.4.0 (from polyfile-weave->weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kaitaistruct~=0.10 (from polyfile-weave->weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading kaitaistruct-0.10-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pdfminer.six<=20240706,>=20220524 (from polyfile-weave->weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "INFO: pip is looking at multiple versions of polyfile-weave to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting polyfile-weave (from weave>=0.51.51->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading polyfile_weave-0.5.5-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting cut_cross_entropy (from unsloth-zoo==2025.8.5->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading cut_cross_entropy-24.12.3-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting torchdata==0.11.0 (from torchtune->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting weave>=0.51.51 (from openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading weave-0.52.3-py3-none-any.whl.metadata (27 kB)\n",
            "INFO: pip is still looking at multiple versions of polyfile-weave to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading weave-0.52.2-py3-none-any.whl.metadata (27 kB)\n",
            "  Downloading weave-0.52.1-py3-none-any.whl.metadata (27 kB)\n",
            "  Downloading weave-0.51.59-py3-none-any.whl.metadata (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading weave-0.51.56-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (0.17.0)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth==2025.8.6->openpipe-art[backend,langgraph]==0.4.9) (4.4.4)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading rich_toolkit-0.15.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading fastapi_cloud_cli-0.1.5-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.21.0->openpipe-art[backend,langgraph]==0.4.9) (5.0.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (3.0.51)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (4.9.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.15.2->openpipe-art==0.4.9->openpipe-art[backend,langgraph]==0.4.9) (0.1.2)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (0.5.0.post1)\n",
            "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9)\n",
            "  Downloading rignore-0.6.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.4.0->nbmake>=1.5.5->openpipe-art[backend,langgraph]==0.4.9) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm<=0.10.0,>=0.9.2->openpipe-art[backend,langgraph]==0.4.9) (2.22)\n",
            "Downloading openpipe_art-0.4.9-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.74.1-py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m145.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m252.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.20.0-py3-none-any.whl (504 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m504.6/504.6 kB\u001b[0m \u001b[31m399.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.8.6-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.8.5-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.7/182.7 kB\u001b[0m \u001b[31m383.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m262.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.29-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m278.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m390.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gql-3.5.3-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m158.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading awscli-1.42.17-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m275.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.17-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m213.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m208.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m200.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nbmake-1.5.5-py3-none-any.whl (12 kB)\n",
            "Downloading polars-1.32.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m232.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Downloading vllm-0.10.0-cp38-abi3-manylinux1_x86_64.whl (386.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m386.6/386.6 MB\u001b[0m \u001b[31m270.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m215.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.10.2-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m373.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.19.0-py3-none-any.whl (39 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m268.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m179.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.2.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.21-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.90.0-py3-none-any.whl (734 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m734.6/734.6 kB\u001b[0m \u001b[31m349.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m153.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weave-0.51.56-py3-none-any.whl (584 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m585.0/585.0 kB\u001b[0m \u001b[31m335.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m352.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m350.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.12-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m187.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.8.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading ray-2.48.0-cp312-cp312-manylinux2014_x86_64.whl (70.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m144.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m317.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m290.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.9/284.9 kB\u001b[0m \u001b[31m298.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m360.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m330.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
            "Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m334.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading tyro-0.9.28-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m209.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m287.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.8-py3-none-any.whl (10 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m153.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m392.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_cloud_cli-0.1.5-py3-none-any.whl (18 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m378.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m163.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m147.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.15.0-py3-none-any.whl (29 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m161.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rignore-0.6.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (949 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m949.7/949.7 kB\u001b[0m \u001b[31m219.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, blake3, uvloop, shtab, setuptools, setproctitle, rsa, rignore, python-json-logger, pycountry, pybase64, polars, partial-json-parser, outlines_core, nvidia-nccl-cu12, nvidia-cudnn-cu12, ninja, msgspec, llvmlite, llguidance, lark, jmespath, jedi, interegular, httptools, graphql-core, gguf, eval-type-backport, docutils, dnspython, diskcache, colorama, cbor2, backoff, astor, watchfiles, triton, numba, gql, email-validator, depyf, botocore, wandb, tyro, torch, s3transfer, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai, lm-format-enforcer, xformers, weave, transformers, torchvision, torchaudio, ray, litellm, fastapi-cloud-cli, fastapi-cli, datasets, cut_cross_entropy, bitsandbytes, awscli, accelerate, xgrammar, trl, openpipe-art, mistral_common, langchain_openai, compressed-tensors, unsloth-zoo, nbmake, unsloth, vllm\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9.1\n",
            "    Uninstalling rsa-4.9.1:\n",
            "      Successfully uninstalled rsa-4.9.1\n",
            "  Attempting uninstall: polars\n",
            "    Found existing installation: polars 1.25.2\n",
            "    Uninstalling polars-1.25.2:\n",
            "      Successfully uninstalled polars-1.25.2\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}